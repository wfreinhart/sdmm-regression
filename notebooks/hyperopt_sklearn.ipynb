{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wfreinhart/sdmm-regression/blob/main/notebooks/hyperopt_sklearn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ttyNufmQz9e"
      },
      "source": [
        "# setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9QGCA6BR7KZ",
        "outputId": "050acc41-c513-4f37-c099-becc7e9bf1bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting bayesian-optimization\n",
            "  Downloading bayesian-optimization-1.2.0.tar.gz (14 kB)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (3.1.0)\n",
            "Building wheels for collected packages: bayesian-optimization\n",
            "  Building wheel for bayesian-optimization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bayesian-optimization: filename=bayesian_optimization-1.2.0-py3-none-any.whl size=11685 sha256=31e7aa02be7e4f4e6433239a47fa2e979ab9e907f3e7c93914219e8f5b12f6ce\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/9b/71/f127d694e02eb40bcf18c7ae9613b88a6be4470f57a8528c5b\n",
            "Successfully built bayesian-optimization\n",
            "Installing collected packages: bayesian-optimization\n",
            "Successfully installed bayesian-optimization-1.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install bayesian-optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmO8yyUiSLFM"
      },
      "outputs": [],
      "source": [
        "from bayes_opt import BayesianOptimization\n",
        "from bayes_opt import SequentialDomainReductionTransformer\n",
        "from bayes_opt.logger import JSONLogger\n",
        "from bayes_opt.event import Events\n",
        "from bayes_opt.util import load_logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZ-8AVPpRn4O"
      },
      "source": [
        "## raw data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Nj1Wk55Rrld"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "with open('data-10-folds.json', 'r') as fid:\n",
        "    buffer = json.load(fid)\n",
        "\n",
        "X = np.array(buffer['X'])\n",
        "y = np.array(buffer['y'])\n",
        "\n",
        "fold_data = []\n",
        "for fold in buffer['folds']:\n",
        "    train_index = np.array(fold['train'])\n",
        "    test_index = np.array(fold['test'])\n",
        "\n",
        "    trainX, testX = X[train_index], X[test_index]\n",
        "    trainy, testy = y[train_index], y[test_index]\n",
        "    \n",
        "    fold_data.append({'train': {'X': trainX, 'y': trainy},\n",
        "                      'test': {'X': testX, 'y': testy}\n",
        "                      })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8F2q3ClRoyI"
      },
      "source": [
        "## k-mer tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqtGugcvRmCQ"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import tqdm.notebook\n",
        "from math import factorial\n",
        "\n",
        "\n",
        "def swap_monomers(seq):\n",
        "    seq = seq.replace('A', 'C')\n",
        "    seq = seq.replace('B', 'A')\n",
        "    seq = seq.replace('C', 'B')\n",
        "    return seq\n",
        "\n",
        "\n",
        "def make_base(degree):\n",
        "    # construct unique numbers of A/B monomers\n",
        "    base = []\n",
        "    for i in range(degree+1):\n",
        "        for j in range(i):\n",
        "            seq = ''.join(['A' for _ in range(i-j)] + ['B' for _ in range(j)])\n",
        "            # print((i, j), seq)\n",
        "            base += [''.join(x) for x in itertools.permutations(seq)]\n",
        "            base += [''.join(x) for x in itertools.permutations(swap_monomers(seq))]\n",
        "    base = sorted(set(base))\n",
        "    pruned_base = []\n",
        "    for b in base:\n",
        "        if b not in pruned_base and b[::-1] not in pruned_base:\n",
        "            pruned_base.append(b)\n",
        "    base = pruned_base\n",
        "    print(f'Finding {len(base)} patterns:', base)\n",
        "\n",
        "    return base\n",
        "\n",
        "\n",
        "def featurize(chain_sequences, base):\n",
        "    X = np.zeros([len(chain_sequences), len(base)])\n",
        "    for i, chain in tqdm.notebook.tqdm(enumerate(chain_sequences), total=len(chain_sequences)):\n",
        "        if 'A' in str(chain):\n",
        "            seq = chain\n",
        "        else:\n",
        "            seq = ''.join(['A' if x == 0 else 'B' for x in chain])\n",
        "        X[i] = 0.5 * (np.array([seq.count(b) for b in base]) + np.array([seq[::-1].count(b) for b in base]))\n",
        "    return X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gE6ggx5OVdjW"
      },
      "source": [
        "## create training fold data with k-mers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "7972ddca7a964286b7cb1d04d06dc2f3",
            "ccb2180da9114216a281b46f6a90b6f2",
            "8df46fa60f1d455f816e9acdcb6ed049",
            "152a76c013cb482281bc7b30030b9350",
            "840300179a9f40b8a6679cac51ce436d",
            "1e026b8b3d224c7c8e50c5bb3c7538b8",
            "3e8ddccb118443478aaae28f3453dc00",
            "fe4da703aa68480a8de49ab2fc8da6e2",
            "3a970583bcfb416484fd1106295d063c",
            "9911ac3319f342a4be657bffcb7e195a",
            "cacbb456760241c4938b10213840bddb"
          ]
        },
        "id": "3rOUqvYvVfxi",
        "outputId": "638e14ba-b65b-48f0-8a88-c0357c3dfe0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finding 1085 patterns: ['A', 'AA', 'AAA', 'AAAA', 'AAAAA', 'AAAAAA', 'AAAAAAA', 'AAAAAAAA', 'AAAAAAAAA', 'AAAAAAAAAA', 'AAAAAAAAAB', 'AAAAAAAAB', 'AAAAAAAABA', 'AAAAAAAABB', 'AAAAAAAB', 'AAAAAAABA', 'AAAAAAABAA', 'AAAAAAABAB', 'AAAAAAABB', 'AAAAAAABBA', 'AAAAAAABBB', 'AAAAAAB', 'AAAAAABA', 'AAAAAABAA', 'AAAAAABAAA', 'AAAAAABAAB', 'AAAAAABAB', 'AAAAAABABA', 'AAAAAABABB', 'AAAAAABB', 'AAAAAABBA', 'AAAAAABBAA', 'AAAAAABBAB', 'AAAAAABBB', 'AAAAAABBBA', 'AAAAAABBBB', 'AAAAAB', 'AAAAABA', 'AAAAABAA', 'AAAAABAAA', 'AAAAABAAAA', 'AAAAABAAAB', 'AAAAABAAB', 'AAAAABAABA', 'AAAAABAABB', 'AAAAABAB', 'AAAAABABA', 'AAAAABABAA', 'AAAAABABAB', 'AAAAABABB', 'AAAAABABBA', 'AAAAABABBB', 'AAAAABB', 'AAAAABBA', 'AAAAABBAA', 'AAAAABBAAA', 'AAAAABBAAB', 'AAAAABBAB', 'AAAAABBABA', 'AAAAABBABB', 'AAAAABBB', 'AAAAABBBA', 'AAAAABBBAA', 'AAAAABBBAB', 'AAAAABBBB', 'AAAAABBBBA', 'AAAAABBBBB', 'AAAAB', 'AAAABA', 'AAAABAA', 'AAAABAAA', 'AAAABAAAA', 'AAAABAAAAB', 'AAAABAAAB', 'AAAABAAABA', 'AAAABAAABB', 'AAAABAAB', 'AAAABAABA', 'AAAABAABAA', 'AAAABAABAB', 'AAAABAABB', 'AAAABAABBA', 'AAAABAABBB', 'AAAABAB', 'AAAABABA', 'AAAABABAA', 'AAAABABAAA', 'AAAABABAAB', 'AAAABABAB', 'AAAABABABA', 'AAAABABABB', 'AAAABABB', 'AAAABABBA', 'AAAABABBAA', 'AAAABABBAB', 'AAAABABBB', 'AAAABABBBA', 'AAAABABBBB', 'AAAABB', 'AAAABBA', 'AAAABBAA', 'AAAABBAAA', 'AAAABBAAAA', 'AAAABBAAAB', 'AAAABBAAB', 'AAAABBAABA', 'AAAABBAABB', 'AAAABBAB', 'AAAABBABA', 'AAAABBABAA', 'AAAABBABAB', 'AAAABBABB', 'AAAABBABBA', 'AAAABBABBB', 'AAAABBB', 'AAAABBBA', 'AAAABBBAA', 'AAAABBBAAA', 'AAAABBBAAB', 'AAAABBBAB', 'AAAABBBABA', 'AAAABBBABB', 'AAAABBBB', 'AAAABBBBA', 'AAAABBBBAA', 'AAAABBBBAB', 'AAAABBBBB', 'AAAABBBBBA', 'AAAABBBBBB', 'AAAB', 'AAABA', 'AAABAA', 'AAABAAA', 'AAABAAAAAB', 'AAABAAAAB', 'AAABAAAABA', 'AAABAAAABB', 'AAABAAAB', 'AAABAAABA', 'AAABAAABAA', 'AAABAAABAB', 'AAABAAABB', 'AAABAAABBA', 'AAABAAABBB', 'AAABAAB', 'AAABAABA', 'AAABAABAA', 'AAABAABAAA', 'AAABAABAAB', 'AAABAABAB', 'AAABAABABA', 'AAABAABABB', 'AAABAABB', 'AAABAABBA', 'AAABAABBAA', 'AAABAABBAB', 'AAABAABBB', 'AAABAABBBA', 'AAABAABBBB', 'AAABAB', 'AAABABA', 'AAABABAA', 'AAABABAAA', 'AAABABAAAB', 'AAABABAAB', 'AAABABAABA', 'AAABABAABB', 'AAABABAB', 'AAABABABA', 'AAABABABAA', 'AAABABABAB', 'AAABABABB', 'AAABABABBA', 'AAABABABBB', 'AAABABB', 'AAABABBA', 'AAABABBAA', 'AAABABBAAA', 'AAABABBAAB', 'AAABABBAB', 'AAABABBABA', 'AAABABBABB', 'AAABABBB', 'AAABABBBA', 'AAABABBBAA', 'AAABABBBAB', 'AAABABBBB', 'AAABABBBBA', 'AAABABBBBB', 'AAABB', 'AAABBA', 'AAABBAA', 'AAABBAAA', 'AAABBAAAAB', 'AAABBAAAB', 'AAABBAAABA', 'AAABBAAABB', 'AAABBAAB', 'AAABBAABA', 'AAABBAABAA', 'AAABBAABAB', 'AAABBAABB', 'AAABBAABBA', 'AAABBAABBB', 'AAABBAB', 'AAABBABA', 'AAABBABAA', 'AAABBABAAB', 'AAABBABAB', 'AAABBABABA', 'AAABBABABB', 'AAABBABB', 'AAABBABBA', 'AAABBABBAA', 'AAABBABBAB', 'AAABBABBB', 'AAABBABBBA', 'AAABBABBBB', 'AAABBB', 'AAABBBA', 'AAABBBAA', 'AAABBBAAA', 'AAABBBAAAB', 'AAABBBAAB', 'AAABBBAABA', 'AAABBBAABB', 'AAABBBAB', 'AAABBBABA', 'AAABBBABAA', 'AAABBBABAB', 'AAABBBABB', 'AAABBBABBA', 'AAABBBABBB', 'AAABBBB', 'AAABBBBA', 'AAABBBBAA', 'AAABBBBAAA', 'AAABBBBAAB', 'AAABBBBAB', 'AAABBBBABA', 'AAABBBBABB', 'AAABBBBB', 'AAABBBBBA', 'AAABBBBBAA', 'AAABBBBBAB', 'AAABBBBBB', 'AAABBBBBBA', 'AAABBBBBBB', 'AAB', 'AABA', 'AABAA', 'AABAAAAAAB', 'AABAAAAAB', 'AABAAAAABA', 'AABAAAAABB', 'AABAAAAB', 'AABAAAABA', 'AABAAAABAA', 'AABAAAABAB', 'AABAAAABB', 'AABAAAABBA', 'AABAAAABBB', 'AABAAAB', 'AABAAABA', 'AABAAABAA', 'AABAAABAAB', 'AABAAABAB', 'AABAAABABA', 'AABAAABABB', 'AABAAABB', 'AABAAABBA', 'AABAAABBAA', 'AABAAABBAB', 'AABAAABBB', 'AABAAABBBA', 'AABAAABBBB', 'AABAAB', 'AABAABA', 'AABAABAA', 'AABAABAAAB', 'AABAABAAB', 'AABAABAABA', 'AABAABAABB', 'AABAABAB', 'AABAABABA', 'AABAABABAA', 'AABAABABAB', 'AABAABABB', 'AABAABABBA', 'AABAABABBB', 'AABAABB', 'AABAABBA', 'AABAABBAA', 'AABAABBAAB', 'AABAABBAB', 'AABAABBABA', 'AABAABBABB', 'AABAABBB', 'AABAABBBA', 'AABAABBBAA', 'AABAABBBAB', 'AABAABBBB', 'AABAABBBBA', 'AABAABBBBB', 'AABAB', 'AABABA', 'AABABAA', 'AABABAAAAB', 'AABABAAAB', 'AABABAAABA', 'AABABAAABB', 'AABABAAB', 'AABABAABA', 'AABABAABAB', 'AABABAABB', 'AABABAABBA', 'AABABAABBB', 'AABABAB', 'AABABABA', 'AABABABAA', 'AABABABAAB', 'AABABABAB', 'AABABABABA', 'AABABABABB', 'AABABABB', 'AABABABBA', 'AABABABBAA', 'AABABABBAB', 'AABABABBB', 'AABABABBBA', 'AABABABBBB', 'AABABB', 'AABABBA', 'AABABBAA', 'AABABBAAAB', 'AABABBAAB', 'AABABBAABA', 'AABABBAABB', 'AABABBAB', 'AABABBABA', 'AABABBABAA', 'AABABBABAB', 'AABABBABB', 'AABABBABBA', 'AABABBABBB', 'AABABBB', 'AABABBBA', 'AABABBBAA', 'AABABBBAAB', 'AABABBBAB', 'AABABBBABA', 'AABABBBABB', 'AABABBBB', 'AABABBBBA', 'AABABBBBAA', 'AABABBBBAB', 'AABABBBBB', 'AABABBBBBA', 'AABABBBBBB', 'AABB', 'AABBA', 'AABBAA', 'AABBAAAAAB', 'AABBAAAAB', 'AABBAAAABA', 'AABBAAAABB', 'AABBAAAB', 'AABBAAABA', 'AABBAAABAB', 'AABBAAABB', 'AABBAAABBA', 'AABBAAABBB', 'AABBAAB', 'AABBAABA', 'AABBAABAAB', 'AABBAABAB', 'AABBAABABA', 'AABBAABABB', 'AABBAABB', 'AABBAABBA', 'AABBAABBAA', 'AABBAABBAB', 'AABBAABBB', 'AABBAABBBA', 'AABBAABBBB', 'AABBAB', 'AABBABA', 'AABBABAAAB', 'AABBABAAB', 'AABBABAABA', 'AABBABAABB', 'AABBABAB', 'AABBABABA', 'AABBABABAB', 'AABBABABB', 'AABBABABBA', 'AABBABABBB', 'AABBABB', 'AABBABBA', 'AABBABBAA', 'AABBABBAAB', 'AABBABBAB', 'AABBABBABA', 'AABBABBABB', 'AABBABBB', 'AABBABBBA', 'AABBABBBAA', 'AABBABBBAB', 'AABBABBBB', 'AABBABBBBA', 'AABBABBBBB', 'AABBB', 'AABBBA', 'AABBBAA', 'AABBBAAAAB', 'AABBBAAAB', 'AABBBAAABA', 'AABBBAAABB', 'AABBBAAB', 'AABBBAABA', 'AABBBAABAB', 'AABBBAABB', 'AABBBAABBA', 'AABBBAABBB', 'AABBBAB', 'AABBBABA', 'AABBBABAAB', 'AABBBABAB', 'AABBBABABA', 'AABBBABABB', 'AABBBABB', 'AABBBABBA', 'AABBBABBAB', 'AABBBABBB', 'AABBBABBBA', 'AABBBABBBB', 'AABBBB', 'AABBBBA', 'AABBBBAA', 'AABBBBAAAB', 'AABBBBAAB', 'AABBBBAABA', 'AABBBBAABB', 'AABBBBAB', 'AABBBBABA', 'AABBBBABAB', 'AABBBBABB', 'AABBBBABBA', 'AABBBBABBB', 'AABBBBB', 'AABBBBBA', 'AABBBBBAA', 'AABBBBBAAB', 'AABBBBBAB', 'AABBBBBABA', 'AABBBBBABB', 'AABBBBBB', 'AABBBBBBA', 'AABBBBBBAA', 'AABBBBBBAB', 'AABBBBBBB', 'AABBBBBBBA', 'AABBBBBBBB', 'AB', 'ABA', 'ABAAAAAAAB', 'ABAAAAAAB', 'ABAAAAAABA', 'ABAAAAAABB', 'ABAAAAAB', 'ABAAAAABA', 'ABAAAAABAB', 'ABAAAAABB', 'ABAAAAABBA', 'ABAAAAABBB', 'ABAAAAB', 'ABAAAABA', 'ABAAAABAAB', 'ABAAAABAB', 'ABAAAABABA', 'ABAAAABABB', 'ABAAAABB', 'ABAAAABBA', 'ABAAAABBAB', 'ABAAAABBB', 'ABAAAABBBA', 'ABAAAABBBB', 'ABAAAB', 'ABAAABA', 'ABAAABAAAB', 'ABAAABAAB', 'ABAAABAABA', 'ABAAABAABB', 'ABAAABAB', 'ABAAABABA', 'ABAAABABAB', 'ABAAABABB', 'ABAAABABBA', 'ABAAABABBB', 'ABAAABB', 'ABAAABBA', 'ABAAABBAAB', 'ABAAABBAB', 'ABAAABBABA', 'ABAAABBABB', 'ABAAABBB', 'ABAAABBBA', 'ABAAABBBAB', 'ABAAABBBB', 'ABAAABBBBA', 'ABAAABBBBB', 'ABAAB', 'ABAABA', 'ABAABAAAAB', 'ABAABAAAB', 'ABAABAAABB', 'ABAABAAB', 'ABAABAABA', 'ABAABAABAB', 'ABAABAABB', 'ABAABAABBA', 'ABAABAABBB', 'ABAABAB', 'ABAABABA', 'ABAABABAAB', 'ABAABABAB', 'ABAABABABA', 'ABAABABABB', 'ABAABABB', 'ABAABABBA', 'ABAABABBAB', 'ABAABABBB', 'ABAABABBBA', 'ABAABABBBB', 'ABAABB', 'ABAABBA', 'ABAABBAAAB', 'ABAABBAAB', 'ABAABBAABA', 'ABAABBAABB', 'ABAABBAB', 'ABAABBABA', 'ABAABBABAB', 'ABAABBABB', 'ABAABBABBA', 'ABAABBABBB', 'ABAABBB', 'ABAABBBA', 'ABAABBBAAB', 'ABAABBBAB', 'ABAABBBABA', 'ABAABBBABB', 'ABAABBBB', 'ABAABBBBA', 'ABAABBBBAB', 'ABAABBBBB', 'ABAABBBBBA', 'ABAABBBBBB', 'ABAB', 'ABABA', 'ABABAAAAAB', 'ABABAAAAB', 'ABABAAAABB', 'ABABAAAB', 'ABABAAABAB', 'ABABAAABB', 'ABABAAABBA', 'ABABAAABBB', 'ABABAAB', 'ABABAABAAB', 'ABABAABAB', 'ABABAABABA', 'ABABAABABB', 'ABABAABB', 'ABABAABBA', 'ABABAABBAB', 'ABABAABBB', 'ABABAABBBA', 'ABABAABBBB', 'ABABAB', 'ABABABA', 'ABABABAAAB', 'ABABABAAB', 'ABABABAABB', 'ABABABAB', 'ABABABABA', 'ABABABABAB', 'ABABABABB', 'ABABABABBA', 'ABABABABBB', 'ABABABB', 'ABABABBA', 'ABABABBAAB', 'ABABABBAB', 'ABABABBABA', 'ABABABBABB', 'ABABABBB', 'ABABABBBA', 'ABABABBBAB', 'ABABABBBB', 'ABABABBBBA', 'ABABABBBBB', 'ABABB', 'ABABBA', 'ABABBAAAAB', 'ABABBAAAB', 'ABABBAAABB', 'ABABBAAB', 'ABABBAABAB', 'ABABBAABB', 'ABABBAABBA', 'ABABBAABBB', 'ABABBAB', 'ABABBABA', 'ABABBABAAB', 'ABABBABAB', 'ABABBABABB', 'ABABBABB', 'ABABBABBA', 'ABABBABBAB', 'ABABBABBB', 'ABABBABBBA', 'ABABBABBBB', 'ABABBB', 'ABABBBA', 'ABABBBAAAB', 'ABABBBAAB', 'ABABBBAABB', 'ABABBBAB', 'ABABBBABA', 'ABABBBABAB', 'ABABBBABB', 'ABABBBABBA', 'ABABBBABBB', 'ABABBBB', 'ABABBBBA', 'ABABBBBAAB', 'ABABBBBAB', 'ABABBBBABA', 'ABABBBBABB', 'ABABBBBB', 'ABABBBBBA', 'ABABBBBBAB', 'ABABBBBBB', 'ABABBBBBBA', 'ABABBBBBBB', 'ABB', 'ABBA', 'ABBAAAAAAB', 'ABBAAAAAB', 'ABBAAAAABB', 'ABBAAAAB', 'ABBAAAABAB', 'ABBAAAABB', 'ABBAAAABBA', 'ABBAAAABBB', 'ABBAAAB', 'ABBAAABAAB', 'ABBAAABAB', 'ABBAAABABB', 'ABBAAABB', 'ABBAAABBA', 'ABBAAABBAB', 'ABBAAABBB', 'ABBAAABBBA', 'ABBAAABBBB', 'ABBAAB', 'ABBAABAAAB', 'ABBAABAAB', 'ABBAABAABB', 'ABBAABAB', 'ABBAABABAB', 'ABBAABABB', 'ABBAABABBA', 'ABBAABABBB', 'ABBAABB', 'ABBAABBA', 'ABBAABBAAB', 'ABBAABBAB', 'ABBAABBABB', 'ABBAABBB', 'ABBAABBBA', 'ABBAABBBAB', 'ABBAABBBB', 'ABBAABBBBA', 'ABBAABBBBB', 'ABBAB', 'ABBABAAAAB', 'ABBABAAAB', 'ABBABAAABB', 'ABBABAAB', 'ABBABAABAB', 'ABBABAABB', 'ABBABAABBB', 'ABBABAB', 'ABBABABAAB', 'ABBABABAB', 'ABBABABABB', 'ABBABABB', 'ABBABABBA', 'ABBABABBAB', 'ABBABABBB', 'ABBABABBBA', 'ABBABABBBB', 'ABBABB', 'ABBABBA', 'ABBABBAAAB', 'ABBABBAAB', 'ABBABBAABB', 'ABBABBAB', 'ABBABBABAB', 'ABBABBABB', 'ABBABBABBA', 'ABBABBABBB', 'ABBABBB', 'ABBABBBA', 'ABBABBBAAB', 'ABBABBBAB', 'ABBABBBABB', 'ABBABBBB', 'ABBABBBBA', 'ABBABBBBAB', 'ABBABBBBB', 'ABBABBBBBA', 'ABBABBBBBB', 'ABBB', 'ABBBA', 'ABBBAAAAAB', 'ABBBAAAAB', 'ABBBAAAABB', 'ABBBAAAB', 'ABBBAAABAB', 'ABBBAAABB', 'ABBBAAABBB', 'ABBBAAB', 'ABBBAABAAB', 'ABBBAABAB', 'ABBBAABABB', 'ABBBAABB', 'ABBBAABBAB', 'ABBBAABBB', 'ABBBAABBBA', 'ABBBAABBBB', 'ABBBAB', 'ABBBABAAAB', 'ABBBABAAB', 'ABBBABAABB', 'ABBBABAB', 'ABBBABABAB', 'ABBBABABB', 'ABBBABABBB', 'ABBBABB', 'ABBBABBAAB', 'ABBBABBAB', 'ABBBABBABB', 'ABBBABBB', 'ABBBABBBA', 'ABBBABBBAB', 'ABBBABBBB', 'ABBBABBBBA', 'ABBBABBBBB', 'ABBBB', 'ABBBBA', 'ABBBBAAAAB', 'ABBBBAAAB', 'ABBBBAAABB', 'ABBBBAAB', 'ABBBBAABAB', 'ABBBBAABB', 'ABBBBAABBB', 'ABBBBAB', 'ABBBBABAAB', 'ABBBBABAB', 'ABBBBABABB', 'ABBBBABB', 'ABBBBABBAB', 'ABBBBABBB', 'ABBBBABBBB', 'ABBBBB', 'ABBBBBA', 'ABBBBBAAAB', 'ABBBBBAAB', 'ABBBBBAABB', 'ABBBBBAB', 'ABBBBBABAB', 'ABBBBBABB', 'ABBBBBABBB', 'ABBBBBB', 'ABBBBBBA', 'ABBBBBBAAB', 'ABBBBBBAB', 'ABBBBBBABB', 'ABBBBBBB', 'ABBBBBBBA', 'ABBBBBBBAB', 'ABBBBBBBB', 'ABBBBBBBBA', 'ABBBBBBBBB', 'B', 'BAAAAAAAAB', 'BAAAAAAAB', 'BAAAAAAABB', 'BAAAAAAB', 'BAAAAAABAB', 'BAAAAAABB', 'BAAAAAABBB', 'BAAAAAB', 'BAAAAABAAB', 'BAAAAABAB', 'BAAAAABABB', 'BAAAAABB', 'BAAAAABBAB', 'BAAAAABBB', 'BAAAAABBBB', 'BAAAAB', 'BAAAABAAAB', 'BAAAABAAB', 'BAAAABAABB', 'BAAAABAB', 'BAAAABABAB', 'BAAAABABB', 'BAAAABABBB', 'BAAAABB', 'BAAAABBAAB', 'BAAAABBAB', 'BAAAABBABB', 'BAAAABBB', 'BAAAABBBAB', 'BAAAABBBB', 'BAAAABBBBB', 'BAAAB', 'BAAABAAAB', 'BAAABAAABB', 'BAAABAAB', 'BAAABAABAB', 'BAAABAABB', 'BAAABAABBB', 'BAAABAB', 'BAAABABAAB', 'BAAABABAB', 'BAAABABABB', 'BAAABABB', 'BAAABABBAB', 'BAAABABBB', 'BAAABABBBB', 'BAAABB', 'BAAABBAAAB', 'BAAABBAAB', 'BAAABBAABB', 'BAAABBAB', 'BAAABBABAB', 'BAAABBABB', 'BAAABBABBB', 'BAAABBB', 'BAAABBBAAB', 'BAAABBBAB', 'BAAABBBABB', 'BAAABBBB', 'BAAABBBBAB', 'BAAABBBBB', 'BAAABBBBBB', 'BAAB', 'BAABAAAABB', 'BAABAAABAB', 'BAABAAABB', 'BAABAAABBB', 'BAABAAB', 'BAABAABAAB', 'BAABAABAB', 'BAABAABABB', 'BAABAABB', 'BAABAABBAB', 'BAABAABBB', 'BAABAABBBB', 'BAABAB', 'BAABABAAB', 'BAABABAABB', 'BAABABAB', 'BAABABABAB', 'BAABABABB', 'BAABABABBB', 'BAABABB', 'BAABABBAAB', 'BAABABBAB', 'BAABABBABB', 'BAABABBB', 'BAABABBBAB', 'BAABABBBB', 'BAABABBBBB', 'BAABB', 'BAABBAAABB', 'BAABBAAB', 'BAABBAABAB', 'BAABBAABB', 'BAABBAABBB', 'BAABBAB', 'BAABBABAB', 'BAABBABABB', 'BAABBABB', 'BAABBABBAB', 'BAABBABBB', 'BAABBABBBB', 'BAABBB', 'BAABBBAAB', 'BAABBBAABB', 'BAABBBAB', 'BAABBBABAB', 'BAABBBABB', 'BAABBBABBB', 'BAABBBB', 'BAABBBBAAB', 'BAABBBBAB', 'BAABBBBABB', 'BAABBBBB', 'BAABBBBBAB', 'BAABBBBBB', 'BAABBBBBBB', 'BAB', 'BABAAAAABB', 'BABAAAABAB', 'BABAAAABB', 'BABAAAABBB', 'BABAAABAB', 'BABAAABABB', 'BABAAABB', 'BABAAABBAB', 'BABAAABBB', 'BABAAABBBB', 'BABAABAABB', 'BABAABAB', 'BABAABABAB', 'BABAABABB', 'BABAABABBB', 'BABAABB', 'BABAABBAB', 'BABAABBABB', 'BABAABBB', 'BABAABBBAB', 'BABAABBBB', 'BABAABBBBB', 'BABAB', 'BABABAAABB', 'BABABAABB', 'BABABAABBB', 'BABABAB', 'BABABABAB', 'BABABABABB', 'BABABABB', 'BABABABBAB', 'BABABABBB', 'BABABABBBB', 'BABABB', 'BABABBAABB', 'BABABBAB', 'BABABBABAB', 'BABABBABB', 'BABABBABBB', 'BABABBB', 'BABABBBAB', 'BABABBBABB', 'BABABBBB', 'BABABBBBAB', 'BABABBBBB', 'BABABBBBBB', 'BABB', 'BABBAAAABB', 'BABBAAABB', 'BABBAAABBB', 'BABBAABABB', 'BABBAABB', 'BABBAABBAB', 'BABBAABBB', 'BABBAABBBB', 'BABBAB', 'BABBABAABB', 'BABBABABB', 'BABBABABBB', 'BABBABB', 'BABBABBAB', 'BABBABBABB', 'BABBABBB', 'BABBABBBAB', 'BABBABBBB', 'BABBABBBBB', 'BABBB', 'BABBBAAABB', 'BABBBAABB', 'BABBBAABBB', 'BABBBAB', 'BABBBABABB', 'BABBBABB', 'BABBBABBB', 'BABBBABBBB', 'BABBBB', 'BABBBBAABB', 'BABBBBAB', 'BABBBBABB', 'BABBBBABBB', 'BABBBBB', 'BABBBBBAB', 'BABBBBBABB', 'BABBBBBB', 'BABBBBBBAB', 'BABBBBBBB', 'BABBBBBBBB', 'BB', 'BBAAAAAABB', 'BBAAAAABB', 'BBAAAAABBB', 'BBAAAABABB', 'BBAAAABB', 'BBAAAABBB', 'BBAAAABBBB', 'BBAAABAABB', 'BBAAABABB', 'BBAAABABBB', 'BBAAABB', 'BBAAABBABB', 'BBAAABBB', 'BBAAABBBB', 'BBAAABBBBB', 'BBAABAABB', 'BBAABAABBB', 'BBAABABABB', 'BBAABABB', 'BBAABABBB', 'BBAABABBBB', 'BBAABB', 'BBAABBAABB', 'BBAABBABB', 'BBAABBABBB', 'BBAABBB', 'BBAABBBABB', 'BBAABBBB', 'BBAABBBBB', 'BBAABBBBBB', 'BBABAAABBB', 'BBABAABABB', 'BBABAABBB', 'BBABAABBBB', 'BBABABABB', 'BBABABABBB', 'BBABABB', 'BBABABBABB', 'BBABABBB', 'BBABABBBB', 'BBABABBBBB', 'BBABB', 'BBABBAABBB', 'BBABBABB', 'BBABBABBB', 'BBABBABBBB', 'BBABBB', 'BBABBBABB', 'BBABBBABBB', 'BBABBBB', 'BBABBBBABB', 'BBABBBBB', 'BBABBBBBB', 'BBABBBBBBB', 'BBB', 'BBBAAAABBB', 'BBBAAABBB', 'BBBAAABBBB', 'BBBAABABBB', 'BBBAABBB', 'BBBAABBBB', 'BBBAABBBBB', 'BBBABABBB', 'BBBABABBBB', 'BBBABBABBB', 'BBBABBB', 'BBBABBBB', 'BBBABBBBB', 'BBBABBBBBB', 'BBBB', 'BBBBAABBBB', 'BBBBABBBB', 'BBBBABBBBB', 'BBBBB', 'BBBBBB', 'BBBBBBB', 'BBBBBBBB', 'BBBBBBBBB', 'BBBBBBBBBB']\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7972ddca7a964286b7cb1d04d06dc2f3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2038 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "base = make_base(degree=10)\n",
        "token_X = featurize(X, base)\n",
        "\n",
        "token_fold_data = []\n",
        "for fold in buffer['folds']:\n",
        "    train_index = fold['train']\n",
        "    test_index = fold['test']\n",
        "\n",
        "    trainX, testX = token_X[train_index], token_X[test_index]\n",
        "    trainy, testy = y[train_index], y[test_index]\n",
        "    \n",
        "    token_fold_data.append({'train': {'X': trainX, 'y': trainy},\n",
        "                            'test': {'X': testX, 'y': testy}\n",
        "                           })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlIVNt29RH3E"
      },
      "source": [
        "# scikit-learn models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grflZ_DSRJNB"
      },
      "outputs": [],
      "source": [
        "from sklearn import linear_model\n",
        "from sklearn import neighbors\n",
        "from sklearn import ensemble\n",
        "\n",
        "models = [linear_model.LinearRegression(),\n",
        "          linear_model.Lasso(),\n",
        "          linear_model.Ridge(),\n",
        "          neighbors.KNeighborsRegressor(),\n",
        "          ensemble.RandomForestRegressor(random_state=0)]\n",
        "\n",
        "for model in models:\n",
        "    print(model)\n",
        "    metrics = train_evaluate_model(model, dataset)\n",
        "    print(metrics)\n",
        "\n",
        "# for model in models:\n",
        "#     plot_parity(model.predict, dataset, name=str(model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvlsHQpwSAsp"
      },
      "source": [
        "## optimize linear models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoS1n1aJSAhS",
        "outputId": "ce48945b-0e31-4e07-d976-8d95b39d8fc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|   iter    |  target   |   alpha   |\n",
            "-------------------------------------\n",
            "| \u001b[0m 1       \u001b[0m | \u001b[0m-2.876   \u001b[0m | \u001b[0m 2.195   \u001b[0m |\n",
            "| \u001b[0m 2       \u001b[0m | \u001b[0m-2.883   \u001b[0m | \u001b[0m 2.861   \u001b[0m |\n",
            "| \u001b[0m 3       \u001b[0m | \u001b[0m-2.878   \u001b[0m | \u001b[0m 2.411   \u001b[0m |\n",
            "| \u001b[95m 4       \u001b[0m | \u001b[95m-2.876   \u001b[0m | \u001b[95m 2.18    \u001b[0m |\n",
            "| \u001b[95m 5       \u001b[0m | \u001b[95m-2.871   \u001b[0m | \u001b[95m 1.695   \u001b[0m |\n",
            "| \u001b[95m 6       \u001b[0m | \u001b[95m-2.862   \u001b[0m | \u001b[95m 0.4356  \u001b[0m |\n",
            "| \u001b[0m 7       \u001b[0m | \u001b[0m-2.862   \u001b[0m | \u001b[0m 0.000665\u001b[0m |\n",
            "| \u001b[0m 8       \u001b[0m | \u001b[0m-2.895   \u001b[0m | \u001b[0m 3.999   \u001b[0m |\n",
            "| \u001b[0m 9       \u001b[0m | \u001b[0m-2.865   \u001b[0m | \u001b[0m 0.9651  \u001b[0m |\n",
            "| \u001b[0m 10      \u001b[0m | \u001b[0m-2.884   \u001b[0m | \u001b[0m 2.958   \u001b[0m |\n",
            "| \u001b[0m 11      \u001b[0m | \u001b[0m-2.866   \u001b[0m | \u001b[0m 1.149   \u001b[0m |\n",
            "| \u001b[0m 12      \u001b[0m | \u001b[0m-2.894   \u001b[0m | \u001b[0m 3.901   \u001b[0m |\n",
            "| \u001b[0m 13      \u001b[0m | \u001b[0m-2.887   \u001b[0m | \u001b[0m 3.206   \u001b[0m |\n",
            "| \u001b[0m 14      \u001b[0m | \u001b[0m-2.876   \u001b[0m | \u001b[0m 2.177   \u001b[0m |\n",
            "| \u001b[0m 15      \u001b[0m | \u001b[0m-2.889   \u001b[0m | \u001b[0m 3.409   \u001b[0m |\n",
            "| \u001b[95m 16      \u001b[0m | \u001b[95m-2.862   \u001b[0m | \u001b[95m 0.3493  \u001b[0m |\n",
            "| \u001b[0m 17      \u001b[0m | \u001b[0m-2.886   \u001b[0m | \u001b[0m 3.152   \u001b[0m |\n",
            "| \u001b[0m 18      \u001b[0m | \u001b[0m-2.883   \u001b[0m | \u001b[0m 2.817   \u001b[0m |\n",
            "| \u001b[0m 19      \u001b[0m | \u001b[0m-2.866   \u001b[0m | \u001b[0m 1.181   \u001b[0m |\n",
            "| \u001b[0m 20      \u001b[0m | \u001b[0m-2.875   \u001b[0m | \u001b[0m 2.133   \u001b[0m |\n",
            "| \u001b[0m 21      \u001b[0m | \u001b[0m-2.885   \u001b[0m | \u001b[0m 3.068   \u001b[0m |\n",
            "| \u001b[0m 22      \u001b[0m | \u001b[0m-2.877   \u001b[0m | \u001b[0m 2.287   \u001b[0m |\n",
            "| \u001b[0m 23      \u001b[0m | \u001b[0m-2.888   \u001b[0m | \u001b[0m 3.361   \u001b[0m |\n",
            "| \u001b[95m 24      \u001b[0m | \u001b[95m-2.862   \u001b[0m | \u001b[95m 0.3229  \u001b[0m |\n",
            "| \u001b[0m 25      \u001b[0m | \u001b[0m-2.868   \u001b[0m | \u001b[0m 1.344   \u001b[0m |\n",
            "| \u001b[0m 26      \u001b[0m | \u001b[0m-2.872   \u001b[0m | \u001b[0m 1.782   \u001b[0m |\n",
            "| \u001b[0m 27      \u001b[0m | \u001b[0m-2.867   \u001b[0m | \u001b[0m 1.258   \u001b[0m |\n",
            "| \u001b[0m 28      \u001b[0m | \u001b[0m-2.887   \u001b[0m | \u001b[0m 3.265   \u001b[0m |\n",
            "| \u001b[0m 29      \u001b[0m | \u001b[0m-2.883   \u001b[0m | \u001b[0m 2.879   \u001b[0m |\n",
            "| \u001b[0m 30      \u001b[0m | \u001b[0m-2.893   \u001b[0m | \u001b[0m 3.801   \u001b[0m |\n",
            "| \u001b[0m 31      \u001b[0m | \u001b[0m-2.893   \u001b[0m | \u001b[0m 3.761   \u001b[0m |\n",
            "| \u001b[0m 32      \u001b[0m | \u001b[0m-2.895   \u001b[0m | \u001b[0m 3.981   \u001b[0m |\n",
            "| \u001b[0m 33      \u001b[0m | \u001b[0m-2.877   \u001b[0m | \u001b[0m 2.292   \u001b[0m |\n",
            "| \u001b[0m 34      \u001b[0m | \u001b[0m-2.878   \u001b[0m | \u001b[0m 2.384   \u001b[0m |\n",
            "| \u001b[0m 35      \u001b[0m | \u001b[0m-2.862   \u001b[0m | \u001b[0m 0.4044  \u001b[0m |\n",
            "| \u001b[0m 36      \u001b[0m | \u001b[0m-2.886   \u001b[0m | \u001b[0m 3.14    \u001b[0m |\n",
            "| \u001b[0m 37      \u001b[0m | \u001b[0m-2.891   \u001b[0m | \u001b[0m 3.587   \u001b[0m |\n",
            "| \u001b[0m 38      \u001b[0m | \u001b[0m-2.867   \u001b[0m | \u001b[0m 1.246   \u001b[0m |\n",
            "| \u001b[0m 39      \u001b[0m | \u001b[0m-2.869   \u001b[0m | \u001b[0m 1.466   \u001b[0m |\n",
            "| \u001b[0m 40      \u001b[0m | \u001b[0m-2.875   \u001b[0m | \u001b[0m 2.067   \u001b[0m |\n",
            "| \u001b[0m 41      \u001b[0m | \u001b[0m-2.89    \u001b[0m | \u001b[0m 3.507   \u001b[0m |\n",
            "| \u001b[0m 42      \u001b[0m | \u001b[0m-2.891   \u001b[0m | \u001b[0m 3.633   \u001b[0m |\n",
            "| \u001b[0m 43      \u001b[0m | \u001b[0m-2.886   \u001b[0m | \u001b[0m 3.108   \u001b[0m |\n",
            "| \u001b[0m 44      \u001b[0m | \u001b[0m-2.862   \u001b[0m | \u001b[0m 0.473   \u001b[0m |\n",
            "| \u001b[0m 45      \u001b[0m | \u001b[0m-2.863   \u001b[0m | \u001b[0m 0.733   \u001b[0m |\n",
            "| \u001b[0m 46      \u001b[0m | \u001b[0m-2.863   \u001b[0m | \u001b[0m 0.6987  \u001b[0m |\n",
            "| \u001b[0m 47      \u001b[0m | \u001b[0m-2.877   \u001b[0m | \u001b[0m 2.31    \u001b[0m |\n",
            "| \u001b[0m 48      \u001b[0m | \u001b[0m-2.885   \u001b[0m | \u001b[0m 3.091   \u001b[0m |\n",
            "| \u001b[0m 49      \u001b[0m | \u001b[0m-2.873   \u001b[0m | \u001b[0m 1.859   \u001b[0m |\n",
            "| \u001b[0m 50      \u001b[0m | \u001b[0m-2.87    \u001b[0m | \u001b[0m 1.641   \u001b[0m |\n",
            "=====================================\n",
            "{'target': -2.861680974271757, 'params': {'alpha': 0.3228549416619044}}\n"
          ]
        }
      ],
      "source": [
        "from sklearn import linear_model\n",
        "\n",
        "\n",
        "this_data = token_fold_data[0]\n",
        "\n",
        "def loss_fn(**params):\n",
        "    # params = {'alpha': np.exp(params['log_alpha'])}\n",
        "\n",
        "    model = linear_model.Ridge(**params)\n",
        "\n",
        "    this_X = this_data['train']['X']\n",
        "    this_y = this_data['train']['y']\n",
        "\n",
        "    train_X = np.vstack([this_X, np.fliplr(this_X)])\n",
        "    train_y = np.vstack([this_y, this_y])\n",
        "\n",
        "    model.fit(train_X, train_y)\n",
        "\n",
        "    test_X = this_data['test']['X']\n",
        "    test_y = this_data['test']['y']\n",
        "\n",
        "    # forward sequences\n",
        "    pred_z_fwd = model.predict(test_X)\n",
        "\n",
        "    # reverse sequences\n",
        "    pred_z_rev = model.predict(np.fliplr(test_X))\n",
        "\n",
        "    # averarage the two predictions\n",
        "    rmse_avg = np.sqrt(np.mean((0.5*(pred_z_fwd + pred_z_rev) - test_y)**2))\n",
        "\n",
        "    return -rmse_avg\n",
        "\n",
        "\n",
        "# pbounds = {'log_alpha': (-8, 8)}\n",
        "pbounds = {'alpha': (0, 4)}\n",
        "bounds_transformer = SequentialDomainReductionTransformer()\n",
        "optimizer = BayesianOptimization(\n",
        "    f=loss_fn,\n",
        "    pbounds=pbounds,\n",
        "    # bounds_transformer = bounds_transformer,\n",
        "    random_state=0\n",
        "    )\n",
        "\n",
        "# load_logs(optimizer, logs=[\"bayes-opt-log-2.json\"]);\n",
        "\n",
        "# logger = JSONLogger(path=\"bayes-opt-lasso.json\")\n",
        "# optimizer.subscribe(Events.OPTIMIZATION_STEP, logger)\n",
        "\n",
        "optimizer.maximize(init_points=5, n_iter=45,\n",
        "                   acq='ei', xi=1e-2,\n",
        "                #    acq='ucb', kappa=4,\n",
        "                   )\n",
        "\n",
        "print(optimizer.max)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0oYbm4cXzqD",
        "outputId": "f1c8d418-b71d-4cfa-e363-bb7acce88f33"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.00033546262790251185"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.exp(optimizer.max['params']['log_alpha'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lu7nTongW-BH",
        "outputId": "723e18f5-7e8b-4cdc-d3c6-5199af2becbc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-2.8622375238163715"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss_fn(**{'alpha': 0})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peOqezCK4kCK",
        "outputId": "ebe51d17-9530-4f68-d85a-cba6e1f46f5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100 loops, best of 5: 3.42 ms per loop\n",
            "The slowest run took 4.00 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "10000 loops, best of 5: 53.6 Âµs per loop\n"
          ]
        }
      ],
      "source": [
        "this_data = fold_data[0]\n",
        "\n",
        "model = linear_model.LinearRegression()\n",
        "\n",
        "this_X = this_data['train']['X']\n",
        "this_y = this_data['train']['y']\n",
        "\n",
        "train_X = np.vstack([this_X, np.fliplr(this_X)])\n",
        "train_y = np.vstack([this_y, this_y])\n",
        "\n",
        "%timeit model.fit(train_X, train_y)\n",
        "\n",
        "test_X = this_data['test']['X']\n",
        "test_y = this_data['test']['y']\n",
        "\n",
        "%timeit model.predict(test_X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sv2YGH7SF9ns"
      },
      "source": [
        "## linear regression for k-mers over folds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pf120XsF_Il",
        "outputId": "171f816d-35a7-42c7-f924-e5dd67fa4632"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.059670594243504 0.14250770876776345\n"
          ]
        }
      ],
      "source": [
        "all_rmse = []\n",
        "\n",
        "for i, this_data in enumerate(token_fold_data):\n",
        "\n",
        "    model = linear_model.LinearRegression()\n",
        "\n",
        "    this_X = this_data['train']['X']\n",
        "    this_y = this_data['train']['y']\n",
        "\n",
        "    train_X = np.vstack([this_X, np.fliplr(this_X)])\n",
        "    train_y = np.vstack([this_y, this_y])\n",
        "\n",
        "    model.fit(train_X, train_y)\n",
        "\n",
        "    test_X = this_data['test']['X']\n",
        "    test_y = this_data['test']['y']\n",
        "\n",
        "    # forward sequences\n",
        "    pred_z_fwd = model.predict(test_X)\n",
        "\n",
        "    # reverse sequences\n",
        "    pred_z_rev = model.predict(np.fliplr(test_X))\n",
        "\n",
        "    # averarage the two predictions\n",
        "    rmse_avg = np.sqrt(np.mean((0.5*(pred_z_fwd + pred_z_rev) - test_y)**2))\n",
        "\n",
        "    all_rmse.append(rmse_avg)\n",
        "\n",
        "print(np.mean(all_rmse), np.std(all_rmse))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUQsf4YrHPcN"
      },
      "source": [
        "Linear: 3.0157332226636155 0.22095644516346405\n",
        "\n",
        "Ridge: 2.780543994395317 0.1583177031987842\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOlaTvhgZ6gG"
      },
      "source": [
        "## k-neighbors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-eHKK_eZ8GG",
        "outputId": "5c92e301-2ae3-458d-acc9-7fd69529c0ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|   iter    |  target   | n_neig... |  weights  |\n",
            "-------------------------------------------------\n",
            "| \u001b[0m 1       \u001b[0m | \u001b[0m-3.733   \u001b[0m | \u001b[0m 70.7    \u001b[0m | \u001b[0m 0.7152  \u001b[0m |\n",
            "| \u001b[95m 2       \u001b[0m | \u001b[95m-3.731   \u001b[0m | \u001b[95m 77.55   \u001b[0m | \u001b[95m 0.5449  \u001b[0m |\n",
            "| \u001b[95m 3       \u001b[0m | \u001b[95m-3.71    \u001b[0m | \u001b[95m 54.8    \u001b[0m | \u001b[95m 0.6459  \u001b[0m |\n",
            "| \u001b[0m 4       \u001b[0m | \u001b[0m-3.715   \u001b[0m | \u001b[0m 56.57   \u001b[0m | \u001b[0m 0.8918  \u001b[0m |\n",
            "| \u001b[0m 5       \u001b[0m | \u001b[0m-3.758   \u001b[0m | \u001b[0m 123.4   \u001b[0m | \u001b[0m 0.3834  \u001b[0m |\n",
            "| \u001b[95m 6       \u001b[0m | \u001b[95m-3.651   \u001b[0m | \u001b[95m 44.58   \u001b[0m | \u001b[95m 0.008413\u001b[0m |\n",
            "| \u001b[95m 7       \u001b[0m | \u001b[95m-3.635   \u001b[0m | \u001b[95m 38.05   \u001b[0m | \u001b[95m 0.0     \u001b[0m |\n",
            "| \u001b[0m 8       \u001b[0m | \u001b[0m-3.659   \u001b[0m | \u001b[0m 22.23   \u001b[0m | \u001b[0m 0.9654  \u001b[0m |\n",
            "| \u001b[0m 9       \u001b[0m | \u001b[0m-5.068   \u001b[0m | \u001b[0m 1.007   \u001b[0m | \u001b[0m 0.2338  \u001b[0m |\n",
            "| \u001b[0m 10      \u001b[0m | \u001b[0m-3.772   \u001b[0m | \u001b[0m 101.7   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
            "| \u001b[0m 11      \u001b[0m | \u001b[0m-3.691   \u001b[0m | \u001b[0m 28.5    \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
            "| \u001b[0m 12      \u001b[0m | \u001b[0m-3.728   \u001b[0m | \u001b[0m 89.86   \u001b[0m | \u001b[0m 0.0     \u001b[0m |\n",
            "| \u001b[0m 13      \u001b[0m | \u001b[0m-3.753   \u001b[0m | \u001b[0m 113.5   \u001b[0m | \u001b[0m 0.04018 \u001b[0m |\n",
            "| \u001b[0m 14      \u001b[0m | \u001b[0m-3.691   \u001b[0m | \u001b[0m 40.51   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
            "| \u001b[95m 15      \u001b[0m | \u001b[95m-3.616   \u001b[0m | \u001b[95m 24.37   \u001b[0m | \u001b[95m 0.01294 \u001b[0m |\n",
            "| \u001b[0m 16      \u001b[0m | \u001b[0m-3.8     \u001b[0m | \u001b[0m 128.0   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
            "| \u001b[0m 17      \u001b[0m | \u001b[0m-3.645   \u001b[0m | \u001b[0m 34.04   \u001b[0m | \u001b[0m 0.0     \u001b[0m |\n",
            "| \u001b[0m 18      \u001b[0m | \u001b[0m-3.687   \u001b[0m | \u001b[0m 63.5    \u001b[0m | \u001b[0m 0.0     \u001b[0m |\n",
            "| \u001b[0m 19      \u001b[0m | \u001b[0m-3.749   \u001b[0m | \u001b[0m 84.53   \u001b[0m | \u001b[0m 0.9928  \u001b[0m |\n",
            "| \u001b[95m 20      \u001b[0m | \u001b[95m-3.613   \u001b[0m | \u001b[95m 21.47   \u001b[0m | \u001b[95m 0.004109\u001b[0m |\n",
            "| \u001b[0m 21      \u001b[0m | \u001b[0m-3.614   \u001b[0m | \u001b[0m 22.9    \u001b[0m | \u001b[0m 0.000182\u001b[0m |\n",
            "| \u001b[0m 22      \u001b[0m | \u001b[0m-3.652   \u001b[0m | \u001b[0m 48.62   \u001b[0m | \u001b[0m 0.007788\u001b[0m |\n",
            "| \u001b[95m 23      \u001b[0m | \u001b[95m-3.588   \u001b[0m | \u001b[95m 18.48   \u001b[0m | \u001b[95m 0.008367\u001b[0m |\n",
            "| \u001b[0m 24      \u001b[0m | \u001b[0m-3.591   \u001b[0m | \u001b[0m 16.16   \u001b[0m | \u001b[0m 0.02934 \u001b[0m |\n",
            "| \u001b[0m 25      \u001b[0m | \u001b[0m-3.764   \u001b[0m | \u001b[0m 95.23   \u001b[0m | \u001b[0m 0.9881  \u001b[0m |\n",
            "| \u001b[0m 26      \u001b[0m | \u001b[0m-3.588   \u001b[0m | \u001b[0m 17.51   \u001b[0m | \u001b[0m 0.005872\u001b[0m |\n",
            "| \u001b[0m 27      \u001b[0m | \u001b[0m-3.634   \u001b[0m | \u001b[0m 16.21   \u001b[0m | \u001b[0m 0.9683  \u001b[0m |\n",
            "| \u001b[0m 28      \u001b[0m | \u001b[0m-3.746   \u001b[0m | \u001b[0m 107.6   \u001b[0m | \u001b[0m 0.000599\u001b[0m |\n",
            "| \u001b[0m 29      \u001b[0m | \u001b[0m-3.798   \u001b[0m | \u001b[0m 118.7   \u001b[0m | \u001b[0m 0.9869  \u001b[0m |\n",
            "| \u001b[0m 30      \u001b[0m | \u001b[0m-3.624   \u001b[0m | \u001b[0m 13.85   \u001b[0m | \u001b[0m 0.003183\u001b[0m |\n",
            "| \u001b[0m 31      \u001b[0m | \u001b[0m-3.691   \u001b[0m | \u001b[0m 46.98   \u001b[0m | \u001b[0m 0.9882  \u001b[0m |\n",
            "| \u001b[0m 32      \u001b[0m | \u001b[0m-3.726   \u001b[0m | \u001b[0m 66.29   \u001b[0m | \u001b[0m 0.9913  \u001b[0m |\n",
            "| \u001b[0m 33      \u001b[0m | \u001b[0m-3.682   \u001b[0m | \u001b[0m 36.15   \u001b[0m | \u001b[0m 0.9825  \u001b[0m |\n",
            "| \u001b[0m 34      \u001b[0m | \u001b[0m-3.705   \u001b[0m | \u001b[0m 81.66   \u001b[0m | \u001b[0m 0.002263\u001b[0m |\n",
            "| \u001b[0m 35      \u001b[0m | \u001b[0m-3.682   \u001b[0m | \u001b[0m 59.65   \u001b[0m | \u001b[0m 0.008312\u001b[0m |\n",
            "| \u001b[0m 36      \u001b[0m | \u001b[0m-3.642   \u001b[0m | \u001b[0m 29.55   \u001b[0m | \u001b[0m 0.01078 \u001b[0m |\n",
            "| \u001b[0m 37      \u001b[0m | \u001b[0m-3.789   \u001b[0m | \u001b[0m 110.5   \u001b[0m | \u001b[0m 0.9986  \u001b[0m |\n",
            "| \u001b[0m 38      \u001b[0m | \u001b[0m-3.733   \u001b[0m | \u001b[0m 98.22   \u001b[0m | \u001b[0m 0.05387 \u001b[0m |\n",
            "| \u001b[0m 39      \u001b[0m | \u001b[0m-3.725   \u001b[0m | \u001b[0m 61.83   \u001b[0m | \u001b[0m 0.9933  \u001b[0m |\n",
            "| \u001b[0m 40      \u001b[0m | \u001b[0m-3.695   \u001b[0m | \u001b[0m 73.92   \u001b[0m | \u001b[0m 0.01257 \u001b[0m |\n",
            "| \u001b[0m 41      \u001b[0m | \u001b[0m-3.626   \u001b[0m | \u001b[0m 27.05   \u001b[0m | \u001b[0m 0.004851\u001b[0m |\n",
            "| \u001b[0m 42      \u001b[0m | \u001b[0m-3.687   \u001b[0m | \u001b[0m 50.94   \u001b[0m | \u001b[0m 0.9786  \u001b[0m |\n",
            "| \u001b[0m 43      \u001b[0m | \u001b[0m-3.69    \u001b[0m | \u001b[0m 68.38   \u001b[0m | \u001b[0m 0.01184 \u001b[0m |\n",
            "| \u001b[0m 44      \u001b[0m | \u001b[0m-3.647   \u001b[0m | \u001b[0m 41.4    \u001b[0m | \u001b[0m 0.008238\u001b[0m |\n",
            "| \u001b[0m 45      \u001b[0m | \u001b[0m-3.716   \u001b[0m | \u001b[0m 86.67   \u001b[0m | \u001b[0m 0.00716 \u001b[0m |\n",
            "| \u001b[0m 46      \u001b[0m | \u001b[0m-3.681   \u001b[0m | \u001b[0m 10.89   \u001b[0m | \u001b[0m 0.9833  \u001b[0m |\n",
            "| \u001b[0m 47      \u001b[0m | \u001b[0m-3.67    \u001b[0m | \u001b[0m 13.46   \u001b[0m | \u001b[0m 0.9736  \u001b[0m |\n",
            "| \u001b[0m 48      \u001b[0m | \u001b[0m-3.655   \u001b[0m | \u001b[0m 52.07   \u001b[0m | \u001b[0m 0.01637 \u001b[0m |\n",
            "| \u001b[0m 49      \u001b[0m | \u001b[0m-3.624   \u001b[0m | \u001b[0m 11.54   \u001b[0m | \u001b[0m 0.001805\u001b[0m |\n",
            "| \u001b[0m 50      \u001b[0m | \u001b[0m-3.598   \u001b[0m | \u001b[0m 19.52   \u001b[0m | \u001b[0m 0.000187\u001b[0m |\n",
            "=================================================\n",
            "-3.5883015495666357 {'n_neighbors': 18.48258682766232, 'weights': 'distance'}\n"
          ]
        }
      ],
      "source": [
        "from sklearn import neighbors\n",
        "\n",
        "\n",
        "prev_results = {}\n",
        "this_data = token_fold_data[0]\n",
        "\n",
        "def loss_fn(**params):\n",
        "    params = {k: int(np.round(v)) for k, v in params.items()}\n",
        "    if params['weights'] > 0.5:\n",
        "        params['weights'] = 'uniform'\n",
        "    else:\n",
        "        params['weights'] = 'distance'\n",
        "\n",
        "    tuple_key = tuple([params[k] for k in sorted(params.keys())])\n",
        "    if tuple_key in prev_results:\n",
        "        return prev_results[tuple_key]\n",
        "\n",
        "    model = neighbors.KNeighborsRegressor(**params)\n",
        "\n",
        "    this_X = this_data['train']['X']\n",
        "    this_y = this_data['train']['y']\n",
        "\n",
        "    train_X = np.vstack([this_X, np.fliplr(this_X)])\n",
        "    train_y = np.vstack([this_y, this_y])\n",
        "\n",
        "    model.fit(train_X, train_y)\n",
        "\n",
        "    test_X = this_data['test']['X']\n",
        "    test_y = this_data['test']['y']\n",
        "\n",
        "    # forward sequences\n",
        "    pred_z_fwd = model.predict(test_X)\n",
        "\n",
        "    # reverse sequences\n",
        "    pred_z_rev = model.predict(np.fliplr(test_X))\n",
        "\n",
        "    # averarage the two predictions\n",
        "    rmse_avg = np.sqrt(np.mean((0.5*(pred_z_fwd + pred_z_rev) - test_y)**2))\n",
        "\n",
        "    # save the results\n",
        "    prev_results[tuple_key] = -rmse_avg\n",
        "\n",
        "    return -rmse_avg\n",
        "\n",
        "\n",
        "pbounds = {'n_neighbors': (1, 128), 'weights': (0, 1)}\n",
        "# pbounds = {'alpha': (0, 4)}\n",
        "bounds_transformer = SequentialDomainReductionTransformer()\n",
        "optimizer = BayesianOptimization(\n",
        "    f=loss_fn,\n",
        "    pbounds=pbounds,\n",
        "    # bounds_transformer = bounds_transformer,\n",
        "    random_state=0\n",
        "    )\n",
        "\n",
        "# load_logs(optimizer, logs=[\"bayes-opt-log-2.json\"]);\n",
        "\n",
        "# logger = JSONLogger(path=\"bayes-opt-lasso.json\")\n",
        "# optimizer.subscribe(Events.OPTIMIZATION_STEP, logger)\n",
        "\n",
        "optimizer.maximize(init_points=5, n_iter=45,\n",
        "                   acq='ei', xi=1e-2,\n",
        "                #    acq='ucb', kappa=4,\n",
        "                   )\n",
        "\n",
        "params = optimizer.max['params']\n",
        "if params['weights'] > 0.5:\n",
        "    params['weights'] = 'uniform'\n",
        "else:\n",
        "    params['weights'] = 'distance'\n",
        "print(optimizer.max['target'], params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAZYiXbhbVQk",
        "outputId": "4852a078-1263-4e7f-babb-313316945dea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-3.592616952617676"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss_fn(**{'weights': 1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vd6d4Ds5So0T",
        "outputId": "ea87838c-9eb8-4720-ee29-fa428f4dbdff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.68340842867823 0.12327479452220283\n"
          ]
        }
      ],
      "source": [
        "from sklearn import neighbors\n",
        "\n",
        "rmse = []\n",
        "for this_data in token_fold_data:\n",
        "    model = neighbors.KNeighborsRegressor(n_neighbors=15, weights='distance')\n",
        "\n",
        "    this_X = this_data['train']['X']\n",
        "    this_y = this_data['train']['y']\n",
        "\n",
        "    train_X = np.vstack([this_X, np.fliplr(this_X)])\n",
        "    train_y = np.vstack([this_y, this_y])\n",
        "\n",
        "    model.fit(train_X, train_y)\n",
        "\n",
        "    test_X = this_data['test']['X']\n",
        "    test_y = this_data['test']['y']\n",
        "\n",
        "    # forward sequences\n",
        "    pred_z_fwd = model.predict(test_X)\n",
        "\n",
        "    # reverse sequences\n",
        "    pred_z_rev = model.predict(np.fliplr(test_X))\n",
        "\n",
        "    # averarage the two predictions\n",
        "    rmse_avg = np.sqrt(np.mean((0.5*(pred_z_fwd + pred_z_rev) - test_y)**2))\n",
        "\n",
        "    rmse.append(rmse_avg)\n",
        "\n",
        "print(np.mean(rmse), np.std(rmse))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDoy_aodTIwE",
        "outputId": "e158b963-3582-4b64-c0c2-0af49560fcb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.662855428853419 0.13203350578962403\n"
          ]
        }
      ],
      "source": [
        "print(np.mean(rmse), np.std(rmse))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUVKP_XPc7bL"
      },
      "source": [
        "## random forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEHvFbyxc8re",
        "outputId": "2505a31f-69c7-4ba0-f8c0-954733c90e9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|   iter    |  target   | max_depth | min_sa... | min_sa... | n_esti... |\n",
            "-------------------------------------------------------------------------\n",
            "| \u001b[0m 1       \u001b[0m | \u001b[0m-3.012   \u001b[0m | \u001b[0m 18.46   \u001b[0m | \u001b[0m 12.01   \u001b[0m | \u001b[0m 10.44   \u001b[0m | \u001b[0m 71.57   \u001b[0m |\n",
            "| \u001b[95m 2       \u001b[0m | \u001b[95m-3.005   \u001b[0m | \u001b[95m 14.71   \u001b[0m | \u001b[95m 11.04   \u001b[0m | \u001b[95m 8.126   \u001b[0m | \u001b[95m 114.6   \u001b[0m |\n",
            "| \u001b[95m 3       \u001b[0m | \u001b[95m-2.924   \u001b[0m | \u001b[95m 30.91   \u001b[0m | \u001b[95m 7.368   \u001b[0m | \u001b[95m 13.08   \u001b[0m | \u001b[95m 69.58   \u001b[0m |\n",
            "| \u001b[0m 4       \u001b[0m | \u001b[0m-3.11    \u001b[0m | \u001b[0m 19.04   \u001b[0m | \u001b[0m 14.96   \u001b[0m | \u001b[0m 2.995   \u001b[0m | \u001b[0m 14.8    \u001b[0m |\n",
            "| \u001b[0m 5       \u001b[0m | \u001b[0m-4.005   \u001b[0m | \u001b[0m 2.607   \u001b[0m | \u001b[0m 13.66   \u001b[0m | \u001b[0m 12.89   \u001b[0m | \u001b[0m 111.9   \u001b[0m |\n",
            "| \u001b[0m 6       \u001b[0m | \u001b[0m-2.982   \u001b[0m | \u001b[0m 29.26   \u001b[0m | \u001b[0m 9.535   \u001b[0m | \u001b[0m 14.06   \u001b[0m | \u001b[0m 70.22   \u001b[0m |\n",
            "| \u001b[95m 7       \u001b[0m | \u001b[95m-2.855   \u001b[0m | \u001b[95m 25.81   \u001b[0m | \u001b[95m 4.156   \u001b[0m | \u001b[95m 10.46   \u001b[0m | \u001b[95m 66.38   \u001b[0m |\n",
            "| \u001b[0m 8       \u001b[0m | \u001b[0m-2.897   \u001b[0m | \u001b[0m 30.16   \u001b[0m | \u001b[0m 5.054   \u001b[0m | \u001b[0m 8.227   \u001b[0m | \u001b[0m 59.35   \u001b[0m |\n",
            "| \u001b[0m 9       \u001b[0m | \u001b[0m-2.862   \u001b[0m | \u001b[0m 21.88   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 12.47   \u001b[0m | \u001b[0m 57.1    \u001b[0m |\n",
            "| \u001b[95m 10      \u001b[0m | \u001b[95m-2.849   \u001b[0m | \u001b[95m 29.34   \u001b[0m | \u001b[95m 2.0     \u001b[0m | \u001b[95m 8.651   \u001b[0m | \u001b[95m 80.29   \u001b[0m |\n",
            "| \u001b[0m 11      \u001b[0m | \u001b[0m-2.852   \u001b[0m | \u001b[0m 32.0    \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 7.114   \u001b[0m | \u001b[0m 94.64   \u001b[0m |\n",
            "| \u001b[0m 12      \u001b[0m | \u001b[0m-2.879   \u001b[0m | \u001b[0m 32.0    \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 7.268   \u001b[0m | \u001b[0m 71.04   \u001b[0m |\n",
            "| \u001b[0m 13      \u001b[0m | \u001b[0m-2.862   \u001b[0m | \u001b[0m 26.91   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 9.895   \u001b[0m | \u001b[0m 89.86   \u001b[0m |\n",
            "| \u001b[0m 14      \u001b[0m | \u001b[0m-2.861   \u001b[0m | \u001b[0m 31.52   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 7.531   \u001b[0m | \u001b[0m 86.58   \u001b[0m |\n",
            "| \u001b[0m 15      \u001b[0m | \u001b[0m-2.857   \u001b[0m | \u001b[0m 27.37   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 9.659   \u001b[0m | \u001b[0m 73.13   \u001b[0m |\n",
            "| \u001b[0m 16      \u001b[0m | \u001b[0m-2.871   \u001b[0m | \u001b[0m 27.56   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 7.744   \u001b[0m | \u001b[0m 83.8    \u001b[0m |\n",
            "| \u001b[0m 17      \u001b[0m | \u001b[0m-2.873   \u001b[0m | \u001b[0m 30.93   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 9.467   \u001b[0m | \u001b[0m 83.05   \u001b[0m |\n",
            "| \u001b[0m 18      \u001b[0m | \u001b[0m-2.865   \u001b[0m | \u001b[0m 30.77   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 7.916   \u001b[0m | \u001b[0m 77.39   \u001b[0m |\n",
            "| \u001b[0m 19      \u001b[0m | \u001b[0m-2.863   \u001b[0m | \u001b[0m 28.04   \u001b[0m | \u001b[0m 2.81    \u001b[0m | \u001b[0m 7.989   \u001b[0m | \u001b[0m 78.2    \u001b[0m |\n",
            "| \u001b[0m 20      \u001b[0m | \u001b[0m-2.858   \u001b[0m | \u001b[0m 28.17   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 9.246   \u001b[0m | \u001b[0m 78.57   \u001b[0m |\n",
            "| \u001b[0m 21      \u001b[0m | \u001b[0m-2.882   \u001b[0m | \u001b[0m 30.38   \u001b[0m | \u001b[0m 2.656   \u001b[0m | \u001b[0m 8.115   \u001b[0m | \u001b[0m 80.88   \u001b[0m |\n",
            "| \u001b[0m 22      \u001b[0m | \u001b[0m-2.852   \u001b[0m | \u001b[0m 28.4    \u001b[0m | \u001b[0m 2.012   \u001b[0m | \u001b[0m 8.247   \u001b[0m | \u001b[0m 75.98   \u001b[0m |\n",
            "| \u001b[0m 23      \u001b[0m | \u001b[0m-2.87    \u001b[0m | \u001b[0m 28.68   \u001b[0m | \u001b[0m 2.011   \u001b[0m | \u001b[0m 8.22    \u001b[0m | \u001b[0m 79.5    \u001b[0m |\n",
            "| \u001b[0m 24      \u001b[0m | \u001b[0m-2.859   \u001b[0m | \u001b[0m 30.1    \u001b[0m | \u001b[0m 2.478   \u001b[0m | \u001b[0m 9.041   \u001b[0m | \u001b[0m 76.65   \u001b[0m |\n",
            "| \u001b[0m 25      \u001b[0m | \u001b[0m-2.865   \u001b[0m | \u001b[0m 28.65   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 9.002   \u001b[0m | \u001b[0m 81.83   \u001b[0m |\n",
            "| \u001b[0m 26      \u001b[0m | \u001b[0m-2.867   \u001b[0m | \u001b[0m 29.95   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 8.967   \u001b[0m | \u001b[0m 79.69   \u001b[0m |\n",
            "| \u001b[0m 27      \u001b[0m | \u001b[0m-2.865   \u001b[0m | \u001b[0m 29.89   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 8.366   \u001b[0m | \u001b[0m 82.95   \u001b[0m |\n",
            "| \u001b[0m 28      \u001b[0m | \u001b[0m-2.849   \u001b[0m | \u001b[0m 28.84   \u001b[0m | \u001b[0m 2.314   \u001b[0m | \u001b[0m 8.907   \u001b[0m | \u001b[0m 80.41   \u001b[0m |\n",
            "| \u001b[0m 29      \u001b[0m | \u001b[0m-2.865   \u001b[0m | \u001b[0m 29.65   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 8.881   \u001b[0m | \u001b[0m 81.11   \u001b[0m |\n",
            "| \u001b[0m 30      \u001b[0m | \u001b[0m-2.874   \u001b[0m | \u001b[0m 29.14   \u001b[0m | \u001b[0m 2.254   \u001b[0m | \u001b[0m 8.858   \u001b[0m | \u001b[0m 78.36   \u001b[0m |\n",
            "| \u001b[0m 31      \u001b[0m | \u001b[0m-2.858   \u001b[0m | \u001b[0m 28.98   \u001b[0m | \u001b[0m 2.033   \u001b[0m | \u001b[0m 8.49    \u001b[0m | \u001b[0m 80.88   \u001b[0m |\n",
            "| \u001b[0m 32      \u001b[0m | \u001b[0m-2.849   \u001b[0m | \u001b[0m 29.03   \u001b[0m | \u001b[0m 2.012   \u001b[0m | \u001b[0m 8.812   \u001b[0m | \u001b[0m 80.2    \u001b[0m |\n",
            "| \u001b[0m 33      \u001b[0m | \u001b[0m-2.849   \u001b[0m | \u001b[0m 29.33   \u001b[0m | \u001b[0m 2.183   \u001b[0m | \u001b[0m 8.505   \u001b[0m | \u001b[0m 79.88   \u001b[0m |\n",
            "| \u001b[0m 34      \u001b[0m | \u001b[0m-2.865   \u001b[0m | \u001b[0m 29.19   \u001b[0m | \u001b[0m 2.167   \u001b[0m | \u001b[0m 8.537   \u001b[0m | \u001b[0m 81.56   \u001b[0m |\n",
            "| \u001b[0m 35      \u001b[0m | \u001b[0m-2.875   \u001b[0m | \u001b[0m 29.57   \u001b[0m | \u001b[0m 2.009   \u001b[0m | \u001b[0m 8.529   \u001b[0m | \u001b[0m 79.44   \u001b[0m |\n",
            "| \u001b[0m 36      \u001b[0m | \u001b[0m-2.849   \u001b[0m | \u001b[0m 29.38   \u001b[0m | \u001b[0m 2.133   \u001b[0m | \u001b[0m 8.76    \u001b[0m | \u001b[0m 80.21   \u001b[0m |\n",
            "| \u001b[0m 37      \u001b[0m | \u001b[0m-2.849   \u001b[0m | \u001b[0m 29.17   \u001b[0m | \u001b[0m 2.114   \u001b[0m | \u001b[0m 8.553   \u001b[0m | \u001b[0m 80.33   \u001b[0m |\n",
            "| \u001b[0m 38      \u001b[0m | \u001b[0m-2.849   \u001b[0m | \u001b[0m 29.18   \u001b[0m | \u001b[0m 2.108   \u001b[0m | \u001b[0m 8.736   \u001b[0m | \u001b[0m 79.62   \u001b[0m |\n",
            "| \u001b[0m 39      \u001b[0m | \u001b[0m-2.862   \u001b[0m | \u001b[0m 29.49   \u001b[0m | \u001b[0m 2.022   \u001b[0m | \u001b[0m 8.579   \u001b[0m | \u001b[0m 80.75   \u001b[0m |\n",
            "| \u001b[0m 40      \u001b[0m | \u001b[0m-2.849   \u001b[0m | \u001b[0m 29.21   \u001b[0m | \u001b[0m 2.004   \u001b[0m | \u001b[0m 8.585   \u001b[0m | \u001b[0m 79.94   \u001b[0m |\n",
            "| \u001b[0m 41      \u001b[0m | \u001b[0m-2.862   \u001b[0m | \u001b[0m 29.22   \u001b[0m | \u001b[0m 2.029   \u001b[0m | \u001b[0m 8.716   \u001b[0m | \u001b[0m 80.75   \u001b[0m |\n",
            "| \u001b[0m 42      \u001b[0m | \u001b[0m-2.849   \u001b[0m | \u001b[0m 29.45   \u001b[0m | \u001b[0m 2.009   \u001b[0m | \u001b[0m 8.701   \u001b[0m | \u001b[0m 79.96   \u001b[0m |\n",
            "| \u001b[0m 43      \u001b[0m | \u001b[0m-2.849   \u001b[0m | \u001b[0m 29.44   \u001b[0m | \u001b[0m 2.064   \u001b[0m | \u001b[0m 8.602   \u001b[0m | \u001b[0m 80.15   \u001b[0m |\n",
            "| \u001b[0m 44      \u001b[0m | \u001b[0m-2.849   \u001b[0m | \u001b[0m 29.25   \u001b[0m | \u001b[0m 2.054   \u001b[0m | \u001b[0m 8.691   \u001b[0m | \u001b[0m 80.01   \u001b[0m |\n",
            "| \u001b[0m 45      \u001b[0m | \u001b[0m-2.849   \u001b[0m | \u001b[0m 29.42   \u001b[0m | \u001b[0m 2.051   \u001b[0m | \u001b[0m 8.686   \u001b[0m | \u001b[0m 80.45   \u001b[0m |\n",
            "| \u001b[0m 46      \u001b[0m | \u001b[0m-2.862   \u001b[0m | \u001b[0m 29.27   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 8.613   \u001b[0m | \u001b[0m 80.53   \u001b[0m |\n",
            "| \u001b[0m 47      \u001b[0m | \u001b[0m-2.849   \u001b[0m | \u001b[0m 29.4    \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 8.683   \u001b[0m | \u001b[0m 80.2    \u001b[0m |\n",
            "| \u001b[0m 48      \u001b[0m | \u001b[0m-2.849   \u001b[0m | \u001b[0m 29.28   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 8.622   \u001b[0m | \u001b[0m 80.13   \u001b[0m |\n",
            "| \u001b[0m 49      \u001b[0m | \u001b[0m-2.849   \u001b[0m | \u001b[0m 29.29   \u001b[0m | \u001b[0m 2.034   \u001b[0m | \u001b[0m 8.676   \u001b[0m | \u001b[0m 80.34   \u001b[0m |\n",
            "| \u001b[0m 50      \u001b[0m | \u001b[0m-2.849   \u001b[0m | \u001b[0m 29.37   \u001b[0m | \u001b[0m 2.001   \u001b[0m | \u001b[0m 8.629   \u001b[0m | \u001b[0m 80.06   \u001b[0m |\n",
            "=========================================================================\n",
            "{'target': -2.8492778527175093, 'params': {'max_depth': 29.336665999093576, 'min_samples_leaf': 2.0, 'min_samples_split': 8.650850637289764, 'n_estimators': 80.29468832899438}}\n"
          ]
        }
      ],
      "source": [
        "from sklearn import ensemble\n",
        "\n",
        "\n",
        "prev_results = {}\n",
        "this_data = token_fold_data[0]\n",
        "\n",
        "named_params = {'bootstrap': [False, True],\n",
        "                'max_features': ['auto', 'sqrt']}\n",
        "\n",
        "def loss_fn(**params):\n",
        "    params = {k: int(np.round(v)) for k, v in params.items()}\n",
        "    for key in named_params.keys():\n",
        "        params[key] = named_params[key][params[key]]\n",
        "\n",
        "    tuple_key = tuple([params[k] for k in sorted(params.keys())])\n",
        "    if tuple_key in prev_results:\n",
        "        return prev_results[tuple_key]\n",
        "\n",
        "    model = ensemble.RandomForestRegressor(**params)\n",
        "\n",
        "    this_X = this_data['train']['X']\n",
        "    this_y = this_data['train']['y']\n",
        "\n",
        "    train_X = np.vstack([this_X, np.fliplr(this_X)])\n",
        "    train_y = np.vstack([this_y, this_y])\n",
        "\n",
        "    model.fit(train_X, train_y)\n",
        "\n",
        "    test_X = this_data['test']['X']\n",
        "    test_y = this_data['test']['y']\n",
        "\n",
        "    # forward sequences\n",
        "    pred_z_fwd = model.predict(test_X)\n",
        "\n",
        "    # reverse sequences\n",
        "    pred_z_rev = model.predict(np.fliplr(test_X))\n",
        "\n",
        "    # averarage the two predictions\n",
        "    rmse_avg = np.sqrt(np.mean((0.5*(pred_z_fwd + pred_z_rev) - test_y)**2))\n",
        "\n",
        "    # save the results\n",
        "    prev_results[tuple_key] = -rmse_avg\n",
        "\n",
        "    return -rmse_avg\n",
        "\n",
        "\n",
        "pbounds = {'max_depth': (2, 32), 'n_estimators': (4, 128),\n",
        "           'min_samples_leaf': (2, 16), 'min_samples_split': (2, 16),\n",
        "           'bootstrap': (0, 1), 'max_features': (0, 1)}\n",
        "bounds_transformer = SequentialDomainReductionTransformer()\n",
        "optimizer = BayesianOptimization(\n",
        "    f=loss_fn,\n",
        "    pbounds=pbounds,\n",
        "    bounds_transformer = bounds_transformer,\n",
        "    random_state=0\n",
        "    )\n",
        "\n",
        "optimizer.maximize(init_points=5, n_iter=45,\n",
        "                   acq='ei', xi=1e-2,\n",
        "                   )\n",
        "\n",
        "print(optimizer.max)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McOzLcI-eYu_",
        "outputId": "16d78a4b-dd61-4123-a56d-94011dfd8f4d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-2.8649303153066294"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss_fn(**{})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nixFFb-i59Sd"
      },
      "source": [
        "# timings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Q9zyiam_3kQL",
        "outputId": "a3a0af9f-45e5-4caf-d558-20ff09013818"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 loop, best of 5: 3min 19s per loop\n",
            "100 loops, best of 5: 2.42 ms per loop\n"
          ]
        }
      ],
      "source": [
        "from sklearn import neural_network\n",
        "\n",
        "this_data = token_fold_data[0]\n",
        "\n",
        "# model = ensemble.RandomForestRegressor()\n",
        "# model = neighbors.KNeighborsRegressor(weights='distance')\n",
        "model = neural_network.MLPRegressor(max_iter=2000, random_state=0)\n",
        "\n",
        "this_X = this_data['train']['X']\n",
        "this_y = this_data['train']['y']\n",
        "\n",
        "train_X = np.vstack([this_X, np.fliplr(this_X)])\n",
        "train_y = np.vstack([this_y, this_y])\n",
        "\n",
        "%timeit model.fit(train_X, train_y)\n",
        "\n",
        "test_X = this_data['test']['X']\n",
        "test_y = this_data['test']['y']\n",
        "\n",
        "%timeit model.predict(test_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0N3534Ik5tcu"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "sdmm-regression/hyperopt-sklearn.ipynb",
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOfYUW0NMGZa1FzaLqbSqwI",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "152a76c013cb482281bc7b30030b9350": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9911ac3319f342a4be657bffcb7e195a",
            "placeholder": "â",
            "style": "IPY_MODEL_cacbb456760241c4938b10213840bddb",
            "value": " 2038/2038 [00:02&lt;00:00, 708.97it/s]"
          }
        },
        "1e026b8b3d224c7c8e50c5bb3c7538b8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a970583bcfb416484fd1106295d063c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3e8ddccb118443478aaae28f3453dc00": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7972ddca7a964286b7cb1d04d06dc2f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ccb2180da9114216a281b46f6a90b6f2",
              "IPY_MODEL_8df46fa60f1d455f816e9acdcb6ed049",
              "IPY_MODEL_152a76c013cb482281bc7b30030b9350"
            ],
            "layout": "IPY_MODEL_840300179a9f40b8a6679cac51ce436d"
          }
        },
        "840300179a9f40b8a6679cac51ce436d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8df46fa60f1d455f816e9acdcb6ed049": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe4da703aa68480a8de49ab2fc8da6e2",
            "max": 2038,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3a970583bcfb416484fd1106295d063c",
            "value": 2038
          }
        },
        "9911ac3319f342a4be657bffcb7e195a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cacbb456760241c4938b10213840bddb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ccb2180da9114216a281b46f6a90b6f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e026b8b3d224c7c8e50c5bb3c7538b8",
            "placeholder": "â",
            "style": "IPY_MODEL_3e8ddccb118443478aaae28f3453dc00",
            "value": "100%"
          }
        },
        "fe4da703aa68480a8de49ab2fc8da6e2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}