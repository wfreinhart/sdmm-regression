{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wfreinhart/sdmm-regression/blob/main/notebooks/generate_histograms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8w-sccUk8ql"
      },
      "source": [
        "# Installs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvZFLoWGk7Ej"
      },
      "source": [
        "Install dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGpL1OdVGcIx",
        "outputId": "32567a31-c1cb-45ce-c9e1-0828145dff17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting MDAnalysis==1.0.0\n",
            "  Downloading MDAnalysis-1.0.0.tar.gz (19.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.6 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from MDAnalysis==1.0.0) (1.21.6)\n",
            "Collecting biopython>=1.71\n",
            "  Downloading biopython-1.79-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 31.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: networkx>=1.0 in /usr/local/lib/python3.7/dist-packages (from MDAnalysis==1.0.0) (2.6.3)\n",
            "Collecting GridDataFormats>=0.4.0\n",
            "  Downloading GridDataFormats-0.7.0-py2.py3-none-any.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 32.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from MDAnalysis==1.0.0) (1.15.0)\n",
            "Collecting mmtf-python>=1.0.0\n",
            "  Downloading mmtf_python-1.1.2-py2.py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.7/dist-packages (from MDAnalysis==1.0.0) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from MDAnalysis==1.0.0) (1.4.1)\n",
            "Requirement already satisfied: matplotlib>=1.5.1 in /usr/local/lib/python3.7/dist-packages (from MDAnalysis==1.0.0) (3.2.2)\n",
            "Collecting mock\n",
            "  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: tqdm>=4.43.0 in /usr/local/lib/python3.7/dist-packages (from MDAnalysis==1.0.0) (4.64.0)\n",
            "Collecting gsd>=1.4.0\n",
            "  Downloading gsd-2.5.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (374 kB)\n",
            "\u001b[K     |████████████████████████████████| 374 kB 48.1 MB/s \n",
            "\u001b[?25hCollecting mrcfile\n",
            "  Downloading mrcfile-1.3.0-py2.py3-none-any.whl (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 21 kB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.1->MDAnalysis==1.0.0) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.1->MDAnalysis==1.0.0) (1.4.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.1->MDAnalysis==1.0.0) (3.0.8)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.1->MDAnalysis==1.0.0) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=1.5.1->MDAnalysis==1.0.0) (4.2.0)\n",
            "Requirement already satisfied: msgpack>=0.5.6 in /usr/local/lib/python3.7/dist-packages (from mmtf-python>=1.0.0->MDAnalysis==1.0.0) (1.0.3)\n",
            "Building wheels for collected packages: MDAnalysis\n",
            "  Building wheel for MDAnalysis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for MDAnalysis: filename=MDAnalysis-1.0.0-cp37-cp37m-linux_x86_64.whl size=4549234 sha256=05e7b8cf06a31df7fbe9ba86ac71fa7d1bcb3f94d20c282940ad090f87ae6d1a\n",
            "  Stored in directory: /root/.cache/pip/wheels/e6/2c/3a/d2b056f1f24662cf00b6a298bf66856addd3fe942d7426717a\n",
            "Successfully built MDAnalysis\n",
            "Installing collected packages: mrcfile, mock, mmtf-python, gsd, GridDataFormats, biopython, MDAnalysis\n",
            "Successfully installed GridDataFormats-0.7.0 MDAnalysis-1.0.0 biopython-1.79 gsd-2.5.2 mmtf-python-1.1.2 mock-4.0.3 mrcfile-1.3.0\n",
            "Requirement already satisfied: numba==0.51.2 in /usr/local/lib/python3.7/dist-packages (0.51.2)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba==0.51.2) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba==0.51.2) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from numba==0.51.2) (1.21.6)\n",
            "Collecting umap-learn\n",
            "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
            "\u001b[K     |████████████████████████████████| 88 kB 3.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.4.1)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (0.51.2)\n",
            "Collecting pynndescent>=0.5\n",
            "  Downloading pynndescent-0.5.7.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 32.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from umap-learn) (4.64.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (0.34.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from pynndescent>=0.5->umap-learn) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->umap-learn) (3.1.0)\n",
            "Building wheels for collected packages: umap-learn, pynndescent\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82829 sha256=43471ad1416fca8e450d2b04a93282cc9cd12f52bc4c5dc031dce185ae88c02a\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/52/a5/1fd9e3e76a7ab34f134c07469cd6f16e27ef3a37aeff1fe821\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.7-py3-none-any.whl size=54286 sha256=59d283de89466f6958acebf3d72dd3f7f53bcea64aebd29e4e215f7a6fb7a830\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/2a/f8/7bd5dcec71bd5c669f6f574db3113513696b98f3f9b51f496c\n",
            "Successfully built umap-learn pynndescent\n",
            "Installing collected packages: pynndescent, umap-learn\n",
            "Successfully installed pynndescent-0.5.7 umap-learn-0.5.3\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import MDAnalysis\n",
        "except:\n",
        "    !pip install MDAnalysis==1.0.0\n",
        "    !pip install numba==0.51.2\n",
        "    !pip install umap-learn\n",
        "    exit()  # need to restart the kernel to access the packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MCWuH0nlFAO"
      },
      "source": [
        "# Definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuuQsnoQk6Pz"
      },
      "source": [
        "Define the atomic environment descriptor model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjW0Wn6NGutD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class GaussianHistogram(nn.Module):\n",
        "    def __init__(self, bins, ranges, sigma, device='cpu'):\n",
        "        super(GaussianHistogram, self).__init__()\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "        # create sigma tensor of appropriate shape\n",
        "        self.sigma = torch.tensor(sigma.reshape(3, 1, 1), dtype=torch.float, device=torch.device(device))\n",
        "\n",
        "        # create bin vectors\n",
        "        self.bins = bins\n",
        "        rmin = torch.tensor(ranges[:, [0]], dtype=torch.float, device=torch.device(device))\n",
        "        rmax = torch.tensor(ranges[:, [1]], dtype=torch.float, device=torch.device(device))\n",
        "        delta = (rmax - rmin) / float(bins)\n",
        "        self.centers = rmin + delta * (torch.arange(bins, device=torch.device(device)).float() + 0.5)\n",
        "        self.delta = delta.reshape(3, 1, 1)\n",
        "\n",
        "        # create centers grid\n",
        "        self.xy = []\n",
        "        for i, j in [(0, 1), (0, 2), (1, 2)]:\n",
        "            xv, yv = torch.meshgrid(self.centers[i], self.centers[j])\n",
        "            xy = torch.vstack([xv.reshape(1, -1), yv.reshape(1, -1)])\n",
        "            self.xy.append(torch.unsqueeze(xy, 2))\n",
        "\n",
        "    def forward(self, x):\n",
        "        eps = 1e-6\n",
        "\n",
        "        # generate the histograms\n",
        "        z = torch.zeros([3, self.bins ** 2], device=self.device)\n",
        "        for k, ij in enumerate([(0, 1), (0, 2), (1, 2)]):\n",
        "            # do the gaussian expansion\n",
        "            y = torch.unsqueeze(x[ij, :], 1) - self.xy[k]\n",
        "            y = torch.exp(-0.5 * (y / self.sigma[[ij]]) ** 2) / (self.sigma[[ij]] * np.sqrt(np.pi * 2)) * \\\n",
        "                self.delta[[ij]]\n",
        "            y = y.prod(dim=0)\n",
        "            z[k] = y.sum(dim=1)\n",
        "\n",
        "        # normalize\n",
        "        z /= torch.unsqueeze(z.sum(dim=-1) + eps, -1)\n",
        "\n",
        "        return z\n",
        "\n",
        "    def to(self, device, *args, **kwargs):\n",
        "        super(GaussianHistogram, self).to(*args, **kwargs)\n",
        "        self.sigma = self.sigma.to(device)\n",
        "        self.delta = self.delta.to(device)\n",
        "        self.xy = [x.to(device) for x in self.xy]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-dP_TVYkzAw"
      },
      "source": [
        "Define the coarse graining procedure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQUUpYqXW6Jv"
      },
      "outputs": [],
      "source": [
        "import MDAnalysis\n",
        "from MDAnalysis.lib import distances, mdamath\n",
        "import numpy as np\n",
        "\n",
        "def read_cg(filename, n_cg, frame=-1):\n",
        "    traj = MDAnalysis.Universe(filename)\n",
        "    traj.trajectory[int(frame)]  # go to end of trajectory\n",
        "    xyz = traj.atoms.positions\n",
        "    _, types = np.unique(traj.atoms.types, return_inverse=True)\n",
        "    box = traj.dimensions\n",
        "\n",
        "    f = distances.transform_RtoS(xyz, box)\n",
        "\n",
        "    f_cg = f[0::n_cg]\n",
        "    types_cg = types[0::n_cg]\n",
        "    for i in range(1, n_cg):\n",
        "        vec = f[i::n_cg] - f[0::n_cg]\n",
        "        vec -= np.round(vec)\n",
        "        f_cg += vec\n",
        "        types_cg += types[i::n_cg]\n",
        "\n",
        "    f_cg -= np.round(f_cg)\n",
        "    xyz_cg = distances.transform_StoR(f_cg, box) + 0.5 * box[:3]\n",
        "    _, types_cg = np.unique(types_cg, return_inverse=True)\n",
        "\n",
        "    return xyz_cg, box, types_cg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dl_lOJjdIAQb"
      },
      "source": [
        "# Process the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88NmjaUvkonV"
      },
      "source": [
        "Mount Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbk-J2CxIBv7",
        "outputId": "41b58454-ea80-4b62-f3b3-0ea1a00ac52c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbeYiQFsLKgt"
      },
      "source": [
        "Define the target set of simulations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reVgY98SLKHZ"
      },
      "outputs": [],
      "source": [
        "target_dir = 'gru-cv-kmeans-v2'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AleIGWrAkr8U"
      },
      "source": [
        "Read the `gsd` files available on Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKC_EuK1IQCg",
        "outputId": "421c32fa-e3d5-43ba-a3d2-339279902507"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200\n",
            "/content/drive/Shareddrives/Polymers-Data/trajectories/gru-cv-kmeans-v2/Npol_500_Nmon_20_seq_AABAABBBABABBAAABAAA_run_0_traj_box_43.1_43.1_43.1_kT_0.5.gsd\n"
          ]
        }
      ],
      "source": [
        "import glob, os\n",
        "\n",
        "drive_prefix = '/content/drive/Shareddrives/Polymers-Data'\n",
        "gsd_files = sorted(glob.glob(os.path.join(drive_prefix, 'trajectories', target_dir, '*.gsd')))\n",
        "print(len(gsd_files))\n",
        "filename = gsd_files[0]\n",
        "print(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vd8Dsf8yRFo"
      },
      "source": [
        "## Calculate histograms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAdxViVwkeUi"
      },
      "source": [
        "For each `gsd` file, load all the particle data and compute the representations of the local atomic environment.\n",
        "Then save the result to Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IB8TjFOpHSYs"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import tqdm\n",
        "\n",
        "traj = MDAnalysis.Universe(gsd_files[0])\n",
        "\n",
        "timesteps = np.arange(len(traj.trajectory))\n",
        "# timesteps = [-1]\n",
        "\n",
        "bins = 12  # don't change this!\n",
        "for j, filename in enumerate(gsd_files):\n",
        "\n",
        "    if os.path.isfile(filename.replace('.gsd', '_H.pkl')):\n",
        "        continue\n",
        "\n",
        "    print('processing {:} of {:}, {:s}'.format(j+1, len(gsd_files), filename))\n",
        "    xyz, box, types = read_cg(filename, n_cg, frame=0)\n",
        "    all_H = np.zeros([len(timesteps), len(xyz), n_species * bins, 3 * bins])\n",
        "\n",
        "    complete = True\n",
        "    for ts, timestep in tqdm.tqdm(enumerate(timesteps), total=len(timesteps)):\n",
        "        try:\n",
        "            xyz, box, types = read_cg(filename, n_cg, frame=timestep)\n",
        "        except:\n",
        "            complete = False\n",
        "            break\n",
        "        pairs, dists = distances.self_capped_distance(xyz, cutoff, box=box)\n",
        "\n",
        "        def neighborhood(i):  # define neighborhood fetching function\n",
        "            idx = np.argwhere(pairs == i)[:, 0]\n",
        "            loc = np.unique(pairs[idx])\n",
        "            f = distances.transform_RtoS(xyz[loc] - xyz[i], box)\n",
        "            f -= np.round(f)\n",
        "            r = distances.transform_StoR(f, box)\n",
        "            t = types[loc]\n",
        "            return r, t.reshape(-1, 1)\n",
        "\n",
        "        # process the histograms\n",
        "        idx = np.arange(xyz.shape[0])\n",
        "        for k, i in enumerate(idx):\n",
        "            r, t = neighborhood(i)\n",
        "            data = torch.tensor(r, dtype=torch.float, requires_grad=False).to(device)\n",
        "            output = model(data)\n",
        "            z = output.to('cpu').detach().numpy()\n",
        "            H = np.hstack([z[y, :].reshape(bins, bins) for y in range(3)])\n",
        "            all_H[ts, k] = H\n",
        "\n",
        "    if complete:\n",
        "        with open(filename.replace('.gsd', '_H.pkl'), 'wb') as fid:\n",
        "            pickle.dump(all_H, fid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pf_GiIKyMUG"
      },
      "source": [
        "## Embed individual atomic environments in UMAP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ou_ZkusDKLj-"
      },
      "source": [
        "This is an annoying workaround a `numba` bytecode incompatibility issue between different operating systems.\n",
        "Usually we would just use `pickle` to serialize the `umap` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvy9HUWYyfCa"
      },
      "outputs": [],
      "source": [
        "import umap\n",
        "import os\n",
        "\n",
        "try:\n",
        "    reducer\n",
        "except:\n",
        "    with open(os.path.join(drive_prefix, 'umap-reducers', 'umap-reducer-AB-1024-data.pkl'), 'rb') as fid:\n",
        "        buffer = pickle.load(fid)\n",
        "\n",
        "    reducer = umap.UMAP(n_components=3, n_neighbors=10, min_dist=0., random_state=0, verbose=False)\n",
        "    reducer.fit(buffer['_raw_data'])\n",
        "    reducer.embedding_ = buffer['embedding_']\n",
        "    reducer.graph_ = buffer['graph_']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18RV_LGIkNtu"
      },
      "source": [
        "Embed the atomic environments from each frame into the same UMAP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quA-aydf1omD",
        "outputId": "31667290-8729-4f3f-c2c7-6ad422d3a016"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embedding 200 histograms from google drive\n"
          ]
        }
      ],
      "source": [
        "hist_files = sorted(glob.glob(os.path.join(drive_prefix, 'trajectories', target_dir, '*_H.pkl')))\n",
        "print('embedding {:} histograms from google drive'.format(len(hist_files)))\n",
        "\n",
        "for j, filename in enumerate(hist_files):\n",
        "    if os.path.isfile(filename.replace('_H.pkl', '_UMAP.pkl')):\n",
        "        continue\n",
        "    \n",
        "    print('processing {:} of {:}, {:s}'.format(j+1, len(hist_files), filename))\n",
        "    \n",
        "    with open(filename, 'rb') as fid:\n",
        "        all_H = pickle.load(fid)\n",
        "\n",
        "    all_U = []\n",
        "    for i, H in tqdm.tqdm(enumerate(all_H), total=len(all_H)):\n",
        "        embedding = reducer.transform(H.reshape(H.shape[0], -1))\n",
        "        all_U.append(embedding)\n",
        "\n",
        "    with open(filename.replace('_H.pkl', '_UMAP.pkl'), 'wb') as fid:\n",
        "        pickle.dump(all_U, fid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KbAfvVkkKt-"
      },
      "source": [
        "Visualize the atomic environment space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeizeIdGU0K2"
      },
      "source": [
        "# Embed snapshots in histogram UMAP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPtOjzFVj_ka"
      },
      "source": [
        "Load the pre-existing histogram UMAP.\n",
        "This is for entire frames."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpO-Ab7MW3NR"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    super_reducer\n",
        "except:\n",
        "    with open(os.path.join(drive_prefix, 'umap-reducers', 'hist-reducer-AB-1024-data.pkl'), 'rb') as fid:\n",
        "        buffer = pickle.load(fid)\n",
        "        \n",
        "    super_reducer = umap.UMAP(n_components=2, n_neighbors=16, min_dist=1, random_state=0, verbose=False)\n",
        "    super_reducer.fit(buffer['_raw_data'])\n",
        "    super_reducer.embedding_ = buffer['embedding_']\n",
        "    super_reducer.graph_ = buffer['graph_']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moiq9I6zkGAs"
      },
      "source": [
        "Define a Gaussian expansion histogram model to use for the embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcowpzD6JREB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = \"cuda:0\"\n",
        "else:\n",
        "  device = \"cpu\"\n",
        "\n",
        "device = torch.device(device)\n",
        "\n",
        "hbins = 36\n",
        "res = [hbins*0.5]*3\n",
        "# ranges = np.vstack([embedding.min(axis=0), embedding.max(axis=0)]).T\n",
        "ranges = np.array([[ 2.193795 , 10.049596 ], [ 2.736186 , 10.067611 ], [ 6.3948064,  9.304425 ]])\n",
        "sigma = np.array([ranges[0, 1]/res[0], ranges[1, 1]/res[1], ranges[2, 1]/res[2]])\n",
        "\n",
        "gh = GaussianHistogram(hbins, ranges, sigma, device=device)\n",
        "gh.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0nY4h7nj7gZ"
      },
      "source": [
        "Generate the histograms (\"fingerprints\") for each snapshot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRtIDnm6YcN_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d488dfba-805b-4fb1-aff7-977b4233718f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [00:07<00:00, 25.47it/s]\n"
          ]
        }
      ],
      "source": [
        "fingerprints = []\n",
        "\n",
        "for j, filename in tqdm.tqdm(enumerate(hist_files), total=len(hist_files)):\n",
        "    \n",
        "    with open(filename.replace('_H.pkl', '_UMAP.pkl'), 'rb') as fid:\n",
        "        all_U = pickle.load(fid)\n",
        "\n",
        "    for lam in all_U:\n",
        "\n",
        "        X = torch.tensor(lam.T, device=device)\n",
        "        yh = gh(X).to('cpu').detach().numpy()\n",
        "        yh = [y.reshape(hbins, hbins).T for y in yh]\n",
        "        yh = np.hstack([np.flipud(y) / y.sum() for y in yh])\n",
        "\n",
        "        fingerprints.append(yh)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jweBRXLQjZCk"
      },
      "source": [
        "Embed the histogram of each frame in the global structure space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8g2aaeFAZDd8"
      },
      "outputs": [],
      "source": [
        "frame_embedding = super_reducer.transform(np.array(fingerprints).reshape(len(fingerprints), -1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICh0GLY1ihfv"
      },
      "source": [
        "# Create a `CSV` file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJO5hAQyihQ4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "if '_id_' in hist_files[0]:\n",
        "    keyword = '_id_'  # for random chains\n",
        "else:\n",
        "    keyword = '_seq_'  # for sequence-specified chains\n",
        "\n",
        "frames = len(all_U)  # frames per trajectory\n",
        "trajs = len(hist_files)  # number of trajectories\n",
        "\n",
        "filename = [hist_files[int(i / frames)].replace(drive_prefix + '/', '').replace('_H.pkl', '.gsd') for i in range(frames * trajs)]\n",
        "frame = [i % frames for i in range(frames * trajs)]\n",
        "sequence = [f.split(keyword)[1].split('_run_')[0] for f in filename]\n",
        "data = pd.DataFrame({'Filename': filename, 'Frame': frame, 'Sequence': sequence,\n",
        "                     'Z0': frame_embedding[:, 0], 'Z1': frame_embedding[:, 1]})\n",
        "\n",
        "# let's save it to the google drive folder:\n",
        "destination_filename = os.path.join(os.path.join(drive_prefix, 'trajectories', target_dir), 'embeddings.csv')\n",
        "data.to_csv(destination_filename)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "sdmm-regression/generate-histograms.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}